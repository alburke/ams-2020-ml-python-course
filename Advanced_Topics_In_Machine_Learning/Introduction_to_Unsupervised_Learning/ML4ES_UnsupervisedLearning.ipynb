{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ML4ES_UnsupervisedLearning.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jXZXmJPY1G2y","colab_type":"text"},"source":["**Notebook creator**: Benjamin A. Toms \\\\\n","**Date**: January 2019 \\\\\n","**Purpose**: AMS 2020 Short Course -- Machine Learning in Python for Environmental Science Problems: Advanced Topics  \\\\\n","**Contact Info**: ben.toms@colostate.edu \\\\\n","**Contact Webpage**: https://sites.google.com/rams.colostate.edu/barnesresearchgroup/home"]},{"cell_type":"markdown","metadata":{"id":"T4Ygxwbl9knx","colab_type":"text"},"source":["\n","\n","---\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ec_t-Bcp1c79","colab_type":"text"},"source":["# Introduction to Unsupervised Learning\n","\n","So, what really is unsupervised learning? There is a lot of excitement around this topic -- what if we can discover physically meaningful patterns within our data without any prior knowledge about what patterns might exist? This capability would be powerful for many reasons. Mainly, we could use machine learning to guide new scientific hypotheses that might not stem directly from our current physics-based theories.\n","\n","Well, unsupervised discovery of patterns within data is in itself nothing new, and in fact has existed within the geoscientific community for decades! With that said, modern machine learning algorithms offer new and exciting ways to discover patterns within data, and extend upon the methods of the previous decades.\n","\n","During this session, we will learn about the basics of a few promising types of unsupervised learning. This is meant to provide you with a resource to further explore these methods for your own research, and to provide a base level of coding proficiency within the methods so you can focus on scientific problems rather than coding syntax.\n","\n","\n","We will cover the following methods:\n","\n","\n","1.   Pattern separation methods (clustering)\n","\n","> * K-means clustering \\\\\n","> * Self-organizing maps\n","\n","2.   Pattern extraction methods (dimensionality reduction)\n","\n","> * Autoencoders\n","\n","I've pasted some references below that you can refer to for additional information. We're going to be focusing on the big ideas of these methods, and not digging into the details of each. So, some additional reading would be helpful if you decide to apply any of these in your own research.\n","\n","#### Resources for pattern separation methods\n","\n","* Wilks, Daniel S. Statistical methods in the atmospheric sciences. Vol. 100. Academic press, 2011.\n","\n","* Prof. Dennis Hartmann's objective analysis course notes: https://atmos.uw.edu/~dennis/552.index\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pf5PSQ185-4M","colab_type":"text"},"source":["\n","\n","---\n","\n","#Installing and importing Packages\n","\n","\n","We will first install packages that we need for this tutorial."]},{"cell_type":"code","metadata":{"id":"GUr9l6lT6BtM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"9a01211b-2673-4907-e3be-be1311ffe2d5","executionInfo":{"status":"ok","timestamp":1578607762872,"user_tz":420,"elapsed":59298,"user":{"displayName":"Ben Toms","photoUrl":"","userId":"08326456974883440290"}}},"source":["!pip install netcdf4 #Package for loading in netcdf4 files\n","!pip install cmocean #Package with beautiful colormaps\n","!pip install minisom #Package for self organizing maps\n","\n","#All of these installs are for installing the \"cartopy\" package, which is helpful for plotting data on the globe\n","!apt-get install libproj-dev proj-data proj-bin\n","!apt-get install libgeos-dev\n","!pip install cython\n","!pip install cartopy"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting netcdf4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/4f/d49fe0c65dea4d2ebfdc602d3e3d2a45a172255c151f4497c43f6d94a5f6/netCDF4-1.5.3-cp36-cp36m-manylinux1_x86_64.whl (4.1MB)\n","\u001b[K     |████████████████████████████████| 4.1MB 2.8MB/s \n","\u001b[?25hCollecting cftime\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/20/15fd894e64f14d6b8afa1cd6dd833c1ea85d1cedb46fdba2d9f8fbc3a924/cftime-1.0.4.2-cp36-cp36m-manylinux1_x86_64.whl (308kB)\n","\u001b[K     |████████████████████████████████| 317kB 50.6MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from netcdf4) (1.17.5)\n","Installing collected packages: cftime, netcdf4\n","Successfully installed cftime-1.0.4.2 netcdf4-1.5.3\n","Collecting cmocean\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/02/d0f19b00b252fd972e3daec05be73aa811091528f21b90442a15d6a96d89/cmocean-2.0-py3-none-any.whl (223kB)\n","\u001b[K     |████████████████████████████████| 225kB 2.8MB/s \n","\u001b[?25hInstalling collected packages: cmocean\n","Successfully installed cmocean-2.0\n","Collecting minisom\n","  Downloading https://files.pythonhosted.org/packages/a8/7b/fa65614c0509aa207829535b8cfc0a04c3f9a573fb5cec2bb5771b42c55a/MiniSom-2.2.3.tar.gz\n","Building wheels for collected packages: minisom\n","  Building wheel for minisom (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for minisom: filename=MiniSom-2.2.3-cp36-none-any.whl size=7550 sha256=506c31e5afb2f3bb312d61725a23caa81377c1b27caf1687c529696e35da741e\n","  Stored in directory: /root/.cache/pip/wheels/35/62/79/4d921062c847ee15a3bb3ac2ea984ed401c8b6b2944f08f697\n","Successfully built minisom\n","Installing collected packages: minisom\n","Successfully installed minisom-2.2.3\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","proj-data is already the newest version (4.9.3-2).\n","proj-data set to manually installed.\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-430\n","Use 'apt autoremove' to remove it.\n","The following NEW packages will be installed:\n","  libproj-dev proj-bin\n","0 upgraded, 2 newly installed, 0 to remove and 7 not upgraded.\n","Need to get 232 kB of archives.\n","After this operation, 1,220 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libproj-dev amd64 4.9.3-2 [199 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 proj-bin amd64 4.9.3-2 [32.3 kB]\n","Fetched 232 kB in 1s (372 kB/s)\n","Selecting previously unselected package libproj-dev:amd64.\n","(Reading database ... 135004 files and directories currently installed.)\n","Preparing to unpack .../libproj-dev_4.9.3-2_amd64.deb ...\n","Unpacking libproj-dev:amd64 (4.9.3-2) ...\n","Selecting previously unselected package proj-bin.\n","Preparing to unpack .../proj-bin_4.9.3-2_amd64.deb ...\n","Unpacking proj-bin (4.9.3-2) ...\n","Setting up libproj-dev:amd64 (4.9.3-2) ...\n","Setting up proj-bin (4.9.3-2) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-430\n","Use 'apt autoremove' to remove it.\n","Suggested packages:\n","  libgdal-doc\n","The following NEW packages will be installed:\n","  libgeos-dev\n","0 upgraded, 1 newly installed, 0 to remove and 7 not upgraded.\n","Need to get 73.1 kB of archives.\n","After this operation, 486 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgeos-dev amd64 3.6.2-1build2 [73.1 kB]\n","Fetched 73.1 kB in 0s (187 kB/s)\n","Selecting previously unselected package libgeos-dev.\n","(Reading database ... 135037 files and directories currently installed.)\n","Preparing to unpack .../libgeos-dev_3.6.2-1build2_amd64.deb ...\n","Unpacking libgeos-dev (3.6.2-1build2) ...\n","Setting up libgeos-dev (3.6.2-1build2) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (0.29.14)\n","Collecting cartopy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/92/fe8838fa8158931906dfc4f16c5c1436b3dd2daf83592645b179581403ad/Cartopy-0.17.0.tar.gz (8.9MB)\n","\u001b[K     |████████████████████████████████| 8.9MB 2.9MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Collecting pyshp>=1.1.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/16/3bf15aa864fb77845fab8007eda22c2bd67bd6c1fd13496df452c8c43621/pyshp-2.1.0.tar.gz (215kB)\n","\u001b[K     |████████████████████████████████| 225kB 39.2MB/s \n","\u001b[?25hRequirement already satisfied: shapely>=1.5.6 in /usr/local/lib/python3.6/dist-packages (from cartopy) (1.6.4.post2)\n","Requirement already satisfied: setuptools>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from cartopy) (42.0.2)\n","Requirement already satisfied: six>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from cartopy) (1.12.0)\n","Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.6/dist-packages (from cartopy) (1.17.5)\n","Building wheels for collected packages: cartopy\n","  Building wheel for cartopy (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for cartopy: filename=Cartopy-0.17.0-cp36-cp36m-linux_x86_64.whl size=9713562 sha256=510df67388d553febec85070ee6ce7c1eed2810c46e6998721fb8d5d2c6999e2\n","  Stored in directory: /root/.cache/pip/wheels/cd/cf/40/539f798f94e921e94fd376a5f9d213a6febe77754c0b187c73\n","Successfully built cartopy\n","Building wheels for collected packages: pyshp\n","  Building wheel for pyshp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyshp: filename=pyshp-2.1.0-cp36-none-any.whl size=32607 sha256=6d4d26007c51a20a11ff3391010d7348f69f0c25fc86a7d7449faba752b0e09a\n","  Stored in directory: /root/.cache/pip/wheels/a6/0c/de/321b5192ad416b328975a2f0385f72c64db4656501eba7cc1a\n","Successfully built pyshp\n","Installing collected packages: pyshp, cartopy\n","Successfully installed cartopy-0.17.0 pyshp-2.1.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5u5dviiu8Dtu","colab_type":"text"},"source":["Now, we'll import some packages that we'll use during various stages of the tutorial. I've broken down each package by what it is useful for."]},{"cell_type":"code","metadata":{"id":"S5FF511y8DO5","colab_type":"code","colab":{}},"source":["#General Python math functions\n","import math\n","\n","#Loading in data (netcdf files)\n","import xarray as xr\n","\n","#Handling data\n","import numpy as np\n","\n","#Plotting figures\n","import matplotlib.pyplot as plt #Main plotting package\n","from matplotlib import rcParams #For changing text properties\n","import cmocean #A package with beautiful colormaps\n","import cartopy #Useful for plotting maps\n","import cartopy.util #Requires separate import\n","\n","#Making neural networks; Ensure we are using tensorflow 1.15\n","%tensorflow_version 1.x\n","import keras\n","\n","#Non-neural network machine learning/unsupervised learning algorithms\n","import sklearn.cluster\n","import scipy.cluster\n","import sklearn.decomposition\n","\n","#Self organizing maps\n","from minisom import MiniSom    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gxq0yHBGLefb","colab_type":"text"},"source":["#Removing Auto-Scroll\n","\n","Output cells will automatically scroll through their entire output unless we use the following code:"]},{"cell_type":"code","metadata":{"id":"IT4TR-3YLcQU","colab_type":"code","colab":{}},"source":["%%javascript\n","IPython.OutputArea.prototype._should_scroll = function(lines) {\n","    return false;\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sthiL_ily0vx","colab_type":"text"},"source":["#Downloading Data\n","\n","We'll now download the data from a remote server and temporarily store it on Google Colab."]},{"cell_type":"code","metadata":{"id":"h7_aIDZT0-ct","colab_type":"code","colab":{}},"source":["!wget http://portal.nersc.gov/project/dasrepo/AGU_ML_Tutorial/sst.mon.mean.trefadj.anom.detrend.1880to2018.nc\n","!wget http://portal.nersc.gov/project/dasrepo/AGU_ML_Tutorial/nino34.long.anom.data.txt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"98UWB4HD9FRz","colab_type":"text"},"source":["\n","\n","\n","#Data Processing (Prior to Training)\n","\n","\n","Okay! Now it's time to start processing our data.\n","\n","We will first load the datasets using xarray and numpy.\n","\n","We will also pre-process the data before training the neural network. Remember that our data has already been processed to remove climatology and any linear trends, so our job here is easy."]},{"cell_type":"code","metadata":{"id":"bKBufcPxAXqp","colab_type":"code","colab":{}},"source":["#Load in the sea-surface temperature data\n","sst_dataset = xr.open_dataset('sst.mon.mean.trefadj.anom.detrend.1880to2018.nc')\n","sst = np.array(sst_dataset['sst'])\n","latitudes = np.array(sst_dataset['lat'])\n","longitudes = np.array(sst_dataset['lon'])\n","years = np.linspace(1880, 2019, 12*139 + 1)[:-1] #Create an array for the year of each sample\n","\n","#Load in the Nino3.4 index\n","nino_34 = np.loadtxt('nino34.long.anom.data.txt')\n","\n","#Extract only the tropical Pacific latitudes and longitudes\n","#   We will use latitudes of 30S to 30N and longitudes of 105E to 300E\n","latitude_min = np.argmin(np.abs(latitudes - 30))\n","latitude_max = np.argmin(np.abs(latitudes - -30))\n","longitude_min = np.argmin(np.abs(longitudes - 105))\n","longitude_max = np.argmin(np.abs(longitudes - 300))\n","\n","sst = sst[:, latitude_min:latitude_max, longitude_min:longitude_max]\n","\n","#Collect the tropical latitudes and longitudes for future plotting\n","latitudes_tropics = latitudes[latitude_min:latitude_max][::2]\n","longitudes_tropics = longitudes[longitude_min:longitude_max][::2]\n","\n","#Sparsify the SST dataset by selecting every other latitude/longitude bin\n","# This reduces the training time of the models, which is fine for our purposes\n","sst = sst[:,::2,::2]\n","\n","#Vectorize the sea-surface temperature data by flattening the matrix along the spatial domains.\n","#   We vectorize the data because the algorithms we will be using all operate on 1-D data\n","sst = sst.reshape(sst.shape[0], sst.shape[-2]*sst.shape[-1])\n","\n","#The sst data has 'nan' values where there is land, but we can not train a network with data that has 'nan values\n","# So, we will replace all 'nan' values with zeros\n","sst[np.isnan(sst)] = 0\n","\n","#Flatten the Nino3.4 time series (it is currently in a year x month matrix), and remove unwanted fields such as column/row labels\n","nino_34 = nino_34[10:-1,1:].flatten()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"moakD2_EzXtU","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"wOObZxPgzQt0","colab_type":"text"},"source":["\n","#Pattern Separation Techniques (Clustering)\n","\n","We will first use clustering techniques to separate our dataset into different regimes of variability. \n","\n","The clustering methods that we will use separate the data into clusters according to the similarity of each sample. These methods are useful for separating our data and leaving the fields in their pre-processed state.\n","\n","We can identify which cluster each sample belongs to, which preserves the information within each individual sample. We can also composite across all samples within each cluster which can help remove some of the ''noise\" if we want to characterize each cluster based on its dominant characteristics.\n"]},{"cell_type":"markdown","metadata":{"id":"NfePIAZ7IMhk","colab_type":"text"},"source":["\n","\n","\n","## K-Means Clustering\n","\n","K-means clustering is one of the most common types of clustering. Briefly, this algorithm separates the samples into a specified number of clusters according to the linear distance between each input sample.\n","\n","The most important details of this method are:\n","\n","\n","*   The user must specify the number of clusters prior to training the model\n","*   The algorithm typically uses euclidian distance (linear distance) between vectors to identify which cluster a sample belongs in\n","*   Because of random initialization of each cluster, k-means is not guaranteed to return the same results each time the algorithm is applied on the same dataset\n","\n","###Visualizations of K-means convergence\n","\n","Here's an animation of how k-means iteratively identifies clusters within a dataset:\n","\n","<br>\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1cbJMAqqFiO7C3FqPh0z2GfT1g-TCUn46)\n","\n","<br>\n","\n","The white X's show the centroids for each cluster, and the plot on the right shows the sum of squares at each iteration. The sum of squares is the squared euclidian distance between each data point and its respective cluster.\n","\n","Because the centroids of the nodes are randomly initialized, we can run into a problem where the initialization does not lead to an accurate solution. An example of that is shown below, where the clusters do not identify the four separate clusters simply due to differences in initialization locations for each cluster:\n","\n","<br>\n","\n","![alt text](https://docs.google.com/uc?export=download&id=18kVJvdTT_2WQHK8e-228s9TOgeIw2lqu)\n","\n","You can visually understand that the algorithm isn't separating the clusters how it should, but you can also see that the sum-of-squares metrix plateaus after the first iteration.\n","\n","The random initialization of k-means is a limitation, and so it is common to run the algorithm multiple times in order to identify the most common solution. The most common solution is then used as the final clustering model.\n","\n","###So, how many clusters should we use?\n","\n","If we don't know how many physically meaningful clusters exist within our data before clustering, then how do we know how many clusters to use in our k-means algorithm? This is a good question, and something that partially comes down to your expertise as a scientist, and partially to rules-of-thumb.\n","\n","One useful rule-of-thumb is called the \"elbow\" technique. \n","\n","In this method, a separate k-means model is trained for a range of numer of clusters, typically ranging from 1 to some upper bound that you define. Then, the summed least squares across all clusters is plotted against the number of clusters used. \n","\n","The \"elbow\" in this plot is the number of clusters at which the decrease in the sum of squares is significantly less after that number than before. If this isn't clear, here's a visual description:\n","\n","<br>\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1Sr6jKeVOiiRE68oxm4Awy4DwAZY6zABP)\n","\n","Image source: https://blog.cambridgespark.com/how-to-determine-the-optimal-number-of-clusters-for-k-means-clustering-14f27070048f\n","\n","###References\n","\n","Here are some references for uses of k-means clustering within atmospheric science:\n","\n","* Clifton, Andrew, and Julie K. Lundquist. \"Data clustering reveals climate impacts on local wind phenomena.\" Journal of Applied Meteorology and Climatology 51.8 (2012): 1547-1557.\n","\n","* Robertson, Andrew W., and Michael Ghil. \"Large-scale weather regimes and local climate over the western United States.\" Journal of Climate 12.6 (1999): 1796-1813.\n","\n","* Steinbach, Michael, et al. \"Discovery of climate indices using clustering.\" Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2003.\n","\n","<br>\n","<br>\n","<br>\n","<br>\n","\n","# **Okay, now we'll delve into some code!**"]},{"cell_type":"markdown","metadata":{"id":"7IUPb1EcI183","colab_type":"text"},"source":["We will first start with a simple clustering approach. We will use two clusters to define the sea-surface temperature patterns within the tropical Pacific. We hope that the clustering will extract the El Nino and La Nina patterns."]},{"cell_type":"code","metadata":{"id":"w-gylqlLBn8H","colab_type":"code","colab":{}},"source":["#We will first create a k-means clustering model using scikit learn\n","kmeans_model = sklearn.cluster.KMeans(n_clusters=2, random_state=10).fit(sst)\n","\n","#And now we will use this k-means clustering model to identify which cluster each of our samples belongs within\n","sst_clusters = kmeans_model.fit_predict(sst)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j5zbFev7JhwB","colab_type":"text"},"source":["We will now plot the Nino3.4 time series against the time series of the clusters and see how similar they are.\n","\n","We will only plot the sign of the Nino3.4 index. We are therefore asking whether the clustering algorithm can identify the sign of ENSO.\n","\n","Warning: The plot is ugly, but it gets the job done!"]},{"cell_type":"code","metadata":{"id":"FOjmQxtxJwOm","colab_type":"code","colab":{}},"source":["#Do some quick pre-processing of the Nino3.4 index here \n","nino_sign = np.sign(nino_34)\n","nino_sign[nino_sign == -1] = 0\n","nino_sign = np.abs(nino_sign - 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I58_YnYaDFY-","colab_type":"code","colab":{}},"source":["#Change all future font colors here (changing to light gray for dark mode)\n","rcParams['text.color'] = '0.75'\n","\n","fig1, ax1 = plt.subplots(figsize=(20,4))\n","\n","ax1.plot(years, nino_sign, color='k', label='True Nino3.4 Sign')\n","ax1.plot(years, sst_clusters, color='r', label='KMeans-Identified Sign')\n","\n","ax1.set_ylabel('Sign of the Nino3.4 index', fontsize=20, color='0.75')\n","ax1.set_xlabel('Year', fontsize=20, color='0.75')\n","\n","ax1.set_yticks([0,1])\n","ax1.set_yticklabels(['-1','1'])\n","\n","ax1.set_xticks(np.arange(1880,2020,20))\n","ax1.set_xticklabels(np.arange(1880,2020,20))\n","\n","ax1.tick_params(labelsize=15, color='0.75', labelcolor='0.75')\n","\n","ax1.set_xlim(1880, 2018)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5c31QDfBOD8E","colab_type":"text"},"source":["In the above plot, we are checking if the black and red lines have the same sign. For the most part, it looks like they do, which is great! This means that the K-Means algorithm is able to identify the sign of ENSO with fairly high accuracy. We can calculate that exact accuracy below:"]},{"cell_type":"code","metadata":{"id":"nNPzbeF8HbAp","colab_type":"code","colab":{}},"source":["#Calculate the number of correctly identified ENSO signs using the clustering method\n","nino_sign = np.sign(nino_34)\n","nino_sign[nino_sign == -1] = 0\n","nino_sign = np.abs(nino_sign - 1)\n","\n","print('The k-means method correctly identifies the sign of ENSO ' + str(np.sum(nino_sign == sst_clusters)/len(nino_sign)*100)[:4] + ' percent of the time.')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dmQwWDLjP-Jc","colab_type":"text"},"source":["## Visualizing the clusters\n","\n","\n","So, now we know that the k-means method identifies the phase of ENSO with the accuracy in the above print statement. Not bad, but remember that we have framed this problem to be as simple as possible for demonstration purposes.\n","\n","Now, let's make some plots of each of the clusters.\n","\n","We will first make composite SST maps for each of the clusters."]},{"cell_type":"code","metadata":{"id":"dNwRJVkqQZ8q","colab_type":"code","colab":{}},"source":["#Collect the indices that are associated with each cluster\n","cluster0_indices = np.argwhere(sst_clusters == 0)[:,0]\n","cluster1_indices = np.argwhere(sst_clusters == 1)[:,0]\n","\n","#Now composite across the indices for each cluster\n","cluster0_composite = np.mean(sst[cluster0_indices], axis=0).reshape(len(latitudes_tropics), len(longitudes_tropics))\n","cluster1_composite = np.mean(sst[cluster1_indices], axis=0).reshape(len(latitudes_tropics), len(longitudes_tropics))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N_YgbWGDSaeJ","colab_type":"code","colab":{}},"source":["#Axes instance for the optimal input \n","fig1 = plt.figure(figsize=(20,10))\n","\n","ax = plt.axes((0.0,0.5,0.5,0.5),  projection=cartopy.crs.Robinson(central_longitude=225))\n","\n","#Contour-fill the optimal input\n","contour_plot_1 = ax.contourf(longitudes_tropics, latitudes_tropics, cluster0_composite, levels=np.linspace(-1,1,100), cmap=cmocean.cm.diff, transform=cartopy.crs.PlateCarree(), extend='both')        \n","\n","#Change some aspects of the figure\n","ax.set_title('Cluster 0 Composite (El Nino)', fontsize=45)\n","ax.add_feature(cartopy.feature.LAND, zorder=1, edgecolor='face', facecolor='0.5')\n","ax.set_extent([np.min(longitudes_tropics), np.max(longitudes_tropics), np.min(latitudes_tropics), np.max(latitudes_tropics)], crs=cartopy.crs.PlateCarree())\n","\n","\n","ax = plt.axes((0.0,0.075,0.5,0.5),  projection=cartopy.crs.Robinson(central_longitude=225))\n","\n","#Contour-fill the optimal input\n","contour_plot_1 = ax.contourf(longitudes_tropics, latitudes_tropics, cluster1_composite, levels=np.linspace(-1,1,100), cmap=cmocean.cm.diff, transform=cartopy.crs.PlateCarree(), extend='both')        \n","\n","#Change some aspects of the figure\n","ax.set_title('Cluster 1 Composite (La Nina)', fontsize=45)\n","ax.add_feature(cartopy.feature.LAND, zorder=1, edgecolor='face', facecolor='0.5')\n","ax.set_extent([np.min(longitudes_tropics), np.max(longitudes_tropics), np.min(latitudes_tropics), np.max(latitudes_tropics)], crs=cartopy.crs.PlateCarree())\n","\n","cax = fig1.add_axes([0.06, 0.05, 0.38, 0.025])\n","cbar = fig1.colorbar(contour_plot_1, cax=cax, orientation='horizontal', ticks=np.linspace(-1,1,3))\n","cbar.ax.tick_params(labelsize=30, color='0.75', labelcolor='0.75')\n","cax.text(x=0.5, y=1.1, s='SST Anomaly ($^{\\circ}$C)', rotation=0, ha='center', va='bottom', fontsize=30)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GwBppZhYWs2N","colab_type":"text"},"source":["Looks good! The K-Means composites look like what we would expect for the El Nino and La Nina cases.\n","\n","## Increasing the Number of Clusters\n","\n","We can further separate the modes of SST variability within the tropics by using more clusters. The interpretation then becomes more complicated, so we will not get into the physical structures the clustering is extracting. But, we can play around with the number of clusters to see what those new clusters do look like."]},{"cell_type":"code","metadata":{"id":"GuzOihHuXLxu","colab_type":"code","colab":{}},"source":["#Set a dummy variable for the number of clusters\n","number_clusters = 4\n","\n","#We will first create a k-means clustering model using scikit learn\n","kmeans_model = sklearn.cluster.KMeans(n_clusters=number_clusters, random_state=10).fit(sst)\n","\n","#And now we will use this k-means clustering model to identify which cluster each of our samples belongs within\n","sst_clusters = kmeans_model.fit_predict(sst)\n","\n","#Composite the SST across all of the clusters\n","cluster_composites = np.zeros((number_clusters, len(latitudes_tropics), len(longitudes_tropics)))\n","for cluster_number in range(number_clusters):\n","  cluster_indices = np.argwhere(sst_clusters == cluster_number)[:,0]\n","  cluster_composite = np.mean(sst[cluster_indices], axis=0).reshape(len(latitudes_tropics), len(longitudes_tropics))\n","  cluster_composites[cluster_number] = cluster_composite"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r2BTc3LSX74z","colab_type":"text"},"source":["And now plotting one of the clusters...\n","\n","You can change the index we are plotting to see what they each look like, if you would like."]},{"cell_type":"code","metadata":{"id":"2wqmNIeiYBmo","colab_type":"code","colab":{}},"source":["cluster_to_plot = 0\n","\n","#Axes instance for the optimal input \n","fig1 = plt.figure(figsize=(20,10))\n","\n","ax = plt.axes((0.0,0.5,0.5,0.5),  projection=cartopy.crs.Robinson(central_longitude=225))\n","\n","#Contour-fill the optimal input\n","contour_plot_1 = ax.contourf(longitudes_tropics, latitudes_tropics, cluster_composites[cluster_to_plot], \n","                             levels=np.linspace(-1,1,100), cmap=cmocean.cm.diff, transform=cartopy.crs.PlateCarree(), extend='both')        \n","\n","#Change some aspects of the figure\n","ax.set_title('Cluster ' + str(cluster_to_plot) + ' Composite', fontsize=45)\n","ax.add_feature(cartopy.feature.LAND, zorder=1, edgecolor='face', facecolor='0.5')\n","ax.set_extent([np.min(longitudes_tropics), np.max(longitudes_tropics), np.min(latitudes_tropics), np.max(latitudes_tropics)], crs=cartopy.crs.PlateCarree())\n","\n","cax = fig1.add_axes([0.06, 0.5, 0.38, 0.025])\n","cbar = fig1.colorbar(contour_plot_1, cax=cax, orientation='horizontal', ticks=np.linspace(-1,1,3))\n","cbar.ax.tick_params(labelsize=30, color='0.75', labelcolor='0.75')\n","cax.text(x=0.5, y=1.1, s='SST Anomaly ($^{\\circ}$C)', rotation=0, ha='center', va='bottom', fontsize=30)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kCjWDWLgJBbu","colab_type":"text"},"source":["\n","\n","---\n","\n","\n","\n","# Hierarchical (Agglomerative) Clustering\n","\n","Hierarchical clustering, which is also sometimes called agglomerative clustering, is a slightly different type of clustering than K-means. In this case, we are interested in identifying a hierarchy of clusters within our data. \n","\n","The hierarchy can be used to identify any number of clusters that the user desires, in increments of 2. This aspect is particularly useful compared to k-means, because we do not need to define the number of clusters prior to training our model.\n","\n","The most important details of this method are:\n","\n","\n","\n","*   The user does not need to specify the number of clusters prior to training the model\n","*   There is no random initialization, and so the clusters will be the same each time the algorithm is applied to a dataset\n","\n","\n","Here is an animation showing how the clusters change depending on how many clusters you decide to select from the hierarchy:\n","\n","<br>\n","\n","![alt text](https://docs.google.com/uc?export=download&id=18o1q_ghFZL83uLzhxzLEQU0EFIbfDNNC)\n","\n","Image source: https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/\n","\n","<br>\n","\n","We can use similar tests to k-means to decide how many clusters we should use. The elbow approach is a viable solution, which is illustrated in the below plot:\n","\n","<br>\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1M8yuvgA8nOZVGeHkwQpJtPMvb_foCZY6)\n","\n","Image source: Wilks, Daniel S. Statistical methods in the atmospheric sciences. Vol. 100. Academic press, 2011. Chapter 15.\n","\n","<br>\n","\n","In this figure, the \"Stage Number\" is the number of clusters we have merged. Because there are 26 samples, there is a maximum of 26 clusters. So, a stage number of 26 implies we have one cluster, 25 implies we have two clusters, and so on. \n","\n","In this example denodrogram, we would cut the stage number at a value of 21, so would keep 6 clusters.\n","\n","\n","###References\n","\n","Here are some example atmospheric science publications that use hierarchical clustering:\n","\n","* Casola, J.H. and J.M. Wallace, 2007: Identifying Weather Regimes in the Wintertime 500-hPa Geopotential Height Field for the Pacific–North American Sector Using a Limited-Contour Clustering Technique. J. Appl. Meteor. Climatol., 46, 1619–1630.\n","\n","* Cheng, X. and J.M. Wallace, 1993: Cluster Analysis of the Northern Hemisphere Wintertime 500-hPa Height Field: Spatial Patterns. J. Atmos. Sci., 50, 2674–2696.\n","\n","* Govender, P., and V. Sivakumar. \"Application of k-means and hierarchical clustering techniques for analysis of air pollution: a review (1980-2019).\" Atmospheric Pollution Research (2019).\n","\n","* Johnson, Aaron, et al. \"Hierarchical cluster analysis of a convection-allowing ensemble during the Hazardous Weather Testbed 2009 Spring Experiment. Part I: Development of the object-oriented cluster analysis method for precipitation fields.\" Monthly weather review 139.12 (2011): 3673-3693.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4kMgdyYeZvTU","colab_type":"text"},"source":["We will now go through the same process we completed for the K-Means example, but this time we will use hierarchical clustering."]},{"cell_type":"code","metadata":{"id":"R34ivjCSaN65","colab_type":"code","colab":{}},"source":["#We will first create a hierarchical clustering model using scikit learn\n","hierarchical_model = sklearn.cluster.AgglomerativeClustering(n_clusters=2).fit(sst)\n","\n","#And now we will use this k-means clustering model to identify which cluster each of our samples belongs within\n","sst_clusters = hierarchical_model.fit_predict(sst)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-gu5qU0kjBDM","colab_type":"text"},"source":["We will now plot the Nino3.4 time series against the time series of the clusters and see how similar they are.\n","\n","We will only plot the sign of the Nino3.4 index. We are therefore asking whether the clustering algorithm can identify the sign of ENSO.\n","\n","Warning: The plot is ugly, but it gets the job done!"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZSlAQh4ojRy3","colab":{}},"source":["#Do some quick pre-processing of the Nino3.4 index here \n","nino_sign = np.sign(nino_34)\n","nino_sign[nino_sign == -1] = 0\n","nino_sign = np.abs(nino_sign - 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iKO4--mbjRy9","colab":{}},"source":["#Change all future font colors here (changing to light gray for dark mode)\n","rcParams['text.color'] = '0.75'\n","\n","fig1, ax1 = plt.subplots(figsize=(20,4))\n","\n","ax1.plot(years, nino_sign, color='k', label='True Nino3.4 Sign')\n","ax1.plot(years, sst_clusters, color='r', label='KMeans-Identified Sign')\n","\n","ax1.set_ylabel('Sign of the Nino3.4 index', fontsize=20, color='0.75')\n","ax1.set_xlabel('Year', fontsize=20, color='0.75')\n","\n","ax1.set_yticks([0,1])\n","ax1.set_yticklabels(['-1','1'])\n","\n","ax1.set_xticks(np.arange(1880,2020,20))\n","ax1.set_xticklabels(np.arange(1880,2020,20))\n","\n","ax1.tick_params(labelsize=15, color='0.75', labelcolor='0.75')\n","\n","ax1.set_xlim(1880, 2018)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bJxAhMfOjnY7"},"source":["In the above plot, we are checking if the black and red lines have the same sign. For the most part, it looks like they do, which is great! This means that the K-Means algorithm is able to identify the sign of ENSO with fairly high accuracy. We can calculate that exact accuracy below:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jZLBaYuNjnY-","colab":{}},"source":["#Calculate the number of correctly identified ENSO signs using the clustering method\n","nino_sign = np.sign(nino_34)\n","nino_sign[nino_sign == -1] = 0\n","nino_sign = np.abs(nino_sign - 1)\n","\n","print('The k-means method correctly identifies the sign of ENSO ' + str(np.sum(nino_sign == sst_clusters)/len(nino_sign)*100)[:4] + ' percent of the time.')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TlnhaIkojnZA"},"source":["So, now we know that the k-means method identifies the phase of ENSO with the accuracy in the above print statement. Not bad, but remember that we have framed this problem to be as simple as possible for demonstration purposes.\n","\n","## Visualizing the Clusters\n","\n","Now, let's make some plots of each of the clusters.\n","\n","We will first make composite SST maps for each of the clusters."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NXE4a7tvjnZB","colab":{}},"source":["#Collect the indices that are associated with each cluster\n","cluster0_indices = np.argwhere(sst_clusters == 0)[:,0]\n","cluster1_indices = np.argwhere(sst_clusters == 1)[:,0]\n","\n","#Now composite across the indices for each cluster\n","cluster0_composite = np.mean(sst[cluster0_indices], axis=0).reshape(len(latitudes_tropics), len(longitudes_tropics))\n","cluster1_composite = np.mean(sst[cluster1_indices], axis=0).reshape(len(latitudes_tropics), len(longitudes_tropics))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ULKN1eIcjnZC","colab":{}},"source":["#Axes instance for the optimal input \n","fig1 = plt.figure(figsize=(20,10))\n","\n","ax = plt.axes((0.0,0.5,0.5,0.5),  projection=cartopy.crs.Robinson(central_longitude=225))\n","\n","#Contour-fill the optimal input\n","contour_plot_1 = ax.contourf(longitudes_tropics, latitudes_tropics, cluster0_composite, levels=np.linspace(-1,1,100), cmap=cmocean.cm.diff, transform=cartopy.crs.PlateCarree(), extend='both')        \n","\n","#Change some aspects of the figure\n","ax.set_title('Cluster 0 Composite (El Nino)', fontsize=45)\n","ax.add_feature(cartopy.feature.LAND, zorder=1, edgecolor='face', facecolor='0.5')\n","ax.set_extent([np.min(longitudes_tropics), np.max(longitudes_tropics), np.min(latitudes_tropics), np.max(latitudes_tropics)], crs=cartopy.crs.PlateCarree())\n","\n","\n","ax = plt.axes((0.0,0.075,0.5,0.5),  projection=cartopy.crs.Robinson(central_longitude=225))\n","\n","#Contour-fill the optimal input\n","contour_plot_1 = ax.contourf(longitudes_tropics, latitudes_tropics, cluster1_composite, levels=np.linspace(-1,1,100), cmap=cmocean.cm.diff, transform=cartopy.crs.PlateCarree(), extend='both')        \n","\n","#Change some aspects of the figure\n","ax.set_title('Cluster 1 Composite (La Nina)', fontsize=45)\n","ax.add_feature(cartopy.feature.LAND, zorder=1, edgecolor='face', facecolor='0.5')\n","ax.set_extent([np.min(longitudes_tropics), np.max(longitudes_tropics), np.min(latitudes_tropics), np.max(latitudes_tropics)], crs=cartopy.crs.PlateCarree())\n","\n","cax = fig1.add_axes([0.06, 0.05, 0.38, 0.025])\n","cbar = fig1.colorbar(contour_plot_1, cax=cax, orientation='horizontal', ticks=np.linspace(-1,1,3))\n","cbar.ax.tick_params(labelsize=30, color='0.75', labelcolor='0.75')\n","cax.text(x=0.5, y=1.1, s='SST Anomaly ($^{\\circ}$C)', rotation=0, ha='center', va='bottom', fontsize=30)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zl0Vz78VkEVl"},"source":["Looks good! The K-Means composites look like what we would expect for the El Nino and La Nina cases.\n","\n","##Increasing the Number of Clusters\n","\n","We can further separate the modes of SST variability within the tropics by using more clusters. The interpretation then becomes more complicated, so we will not get into the physical structures the clustering is extracting. But, we can play around with the number of clusters to see what those new clusters do look like."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JP6NYSRFkEVn","colab":{}},"source":["#Set a dummy variable for the number of clusters\n","number_clusters = 8\n","\n","#We will first create a k-means clustering model using scikit learn\n","hierarchical_model = sklearn.cluster.AgglomerativeClustering(n_clusters=number_clusters).fit(sst)\n","\n","#And now we will use this k-means clustering model to identify which cluster each of our samples belongs within\n","sst_clusters = hierarchical_model.fit_predict(sst)  \n","\n","#Composite the SST across all of the clusters\n","cluster_composites = np.zeros((number_clusters, len(latitudes_tropics), len(longitudes_tropics)))\n","for cluster_number in range(number_clusters):\n","  cluster_indices = np.argwhere(sst_clusters == cluster_number)[:,0]\n","  cluster_composite = np.mean(sst[cluster_indices], axis=0).reshape(len(latitudes_tropics), len(longitudes_tropics))\n","  cluster_composites[cluster_number] = cluster_composite"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uSvsQ7Z-kEVq"},"source":["And now plotting one of the clusters...\n","\n","You can change the index we are plotting to see what they each look like, if you would like."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2N47O7EIkEVr","colab":{}},"source":["cluster_to_plot = 3\n","\n","#Axes instance for the optimal input \n","fig1 = plt.figure(figsize=(20,10))\n","\n","ax = plt.axes((0.0,0.5,0.5,0.5),  projection=cartopy.crs.Robinson(central_longitude=225))\n","\n","#Contour-fill the optimal input\n","contour_plot_1 = ax.contourf(longitudes_tropics, latitudes_tropics, cluster_composites[cluster_to_plot], \n","                             levels=np.linspace(-1,1,100), cmap=cmocean.cm.diff, transform=cartopy.crs.PlateCarree(), extend='both')        \n","\n","#Change some aspects of the figure\n","ax.set_title('Cluster ' + str(cluster_to_plot) + ' Composite', fontsize=45)\n","ax.add_feature(cartopy.feature.LAND, zorder=1, edgecolor='face', facecolor='0.5')\n","ax.set_extent([np.min(longitudes_tropics), np.max(longitudes_tropics), np.min(latitudes_tropics), np.max(latitudes_tropics)], crs=cartopy.crs.PlateCarree())\n","\n","cax = fig1.add_axes([0.06, 0.5, 0.38, 0.025])\n","cbar = fig1.colorbar(contour_plot_1, cax=cax, orientation='horizontal', ticks=np.linspace(-1,1,3))\n","cbar.ax.tick_params(labelsize=30, color='0.75', labelcolor='0.75')\n","cax.text(x=0.5, y=1.1, s='SST Anomaly ($^{\\circ}$C)', rotation=0, ha='center', va='bottom', fontsize=30)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zcuGsm0TkHpc","colab_type":"text"},"source":["The scipy package also has a neat utility that allows you to plot the hierarchical dendrogram, which shows how many samples are within each cluster. The dendrogram can be a useful way to determine how many clusters to retain. \n"]},{"cell_type":"code","metadata":{"id":"JHZJZn6sckbB","colab_type":"code","colab":{}},"source":["dendrogram_matrix = scipy.cluster.hierarchy.linkage(sst, 'ward')\n","\n","plt.figure(figsize=(8,8))\n","dendrogram = scipy.cluster.hierarchy.dendrogram(dendrogram_matrix, truncate_mode='level', p=4)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"imJckmM8ixG3","colab_type":"text"},"source":["# Self Organizing Maps\n","\n","Another interesting method for pattern separation is self organizing maps (SOM). SOMs are a type of neural network, and their origins are traced back to the 1970s. Lots of research was done using SOMs back in the late 1990's and early 2000's, but there has been a resurgence in their usage with the new era of interest in the popularity of neural networks.\n","\n","Self organizing maps can be thought of differently than K-means and hierarchical clustering. While K-means and hierarchical clustering identify clusters that correspond with at least one data point, self organizing maps organize a \"map\" of nodes across all samples, regardless of whether data is associated with each node or not.\n","\n","This mapping is advantageous because it helps visualize a continuity of similarities between each cluster. But, it is also disadvantageous, because the user must be aware that some of the clusters might not actually be associated with any data and might represent an interpolation between two clusters that correspond with data. \n","\n","An animation of how self-organizing maps evolve throughout training is shown below.\n","\n","<br>\n","\n","![alt text](https://docs.google.com/uc?export=download&id=10e9LvD7G6awW8bChZlZMtY5XT-9lg6kt)\n","\n","Image source: https://commons.wikimedia.org/wiki/File:2D_data_training_SOM.gif\n","\n","<br>\n","\n","It is typical for maps to be initiated in a rectangular shape. The nodes of the map are \"pulled\" towards the data to minimize a metric, typically the euclidian distance between the data points and the nodes.\n","\n","###References\n","\n","Here are some resources for self-organizing maps and their usage in atmospheric science:\n","\n","* Liu, Yonggang, and Robert H. Weisberg. \"A review of self-organizing map applications in meteorology and oceanography.\" (2011): 253.\n","\n","* Liu, Yonggang, Robert H. Weisberg, and Christopher NK Mooers. \"Performance evaluation of the self‐organizing map for feature extraction.\" Journal of Geophysical Research: Oceans 111.C5 (2006).\n","\n","* Horton, Daniel E., et al. \"Contribution of changes in atmospheric circulation patterns to extreme temperature trends.\" Nature 522.7557 (2015): 465."]},{"cell_type":"markdown","metadata":{"id":"LBruJNPWw0w-","colab_type":"text"},"source":["##Training a Self-Organizing Map\n","\n","We'll train a self organizing map on the sea-surface temperature data."]},{"cell_type":"code","metadata":{"id":"sQI3lQRpsJkL","colab_type":"code","colab":{}},"source":["#Define the self-organizing map with:\n","number_x_nodes = 3 #the number of nodes in the x direction\n","number_y_nodes = 3 #the number of nodes in the y direction\n","input_len = sst.shape[-1] #the number of sst lat/lon locations\n","\n","self_organizing_map = MiniSom(x=number_x_nodes, y=number_y_nodes, input_len=input_len, sigma=0.3, learning_rate=0.5) # initialization of 6x6 SOM\n","\n","#Train the self-organizing map\n","self_organizing_map.train_random(sst, 1000) # trains the SOM with 100 iterations"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MRexjGBbxG5R","colab_type":"text"},"source":["We'll now collect the maps for each cluster and plot them."]},{"cell_type":"code","metadata":{"id":"eOj9wgUOuX3Z","colab_type":"code","colab":{}},"source":["#collect the sea-surface temperature maps for each cluster\n","som_maps = self_organizing_map.get_weights().reshape(number_x_nodes, number_y_nodes, len(latitudes_tropics), len(longitudes_tropics))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yYQ3o2g1xLjU","colab_type":"text"},"source":["And now the plotting code..."]},{"cell_type":"code","metadata":{"id":"R0xz7_inxMzI","colab_type":"code","colab":{}},"source":["x_node_to_plot = 2 #Define the x node to plot\n","y_node_to_plot = 2 #Define the y node to plot\n","\n","#Axes instance for the optimal input \n","fig1 = plt.figure(figsize=(20,10))\n","\n","ax = plt.axes((0.0,0.5,0.5,0.5),  projection=cartopy.crs.Robinson(central_longitude=225))\n","\n","#Contour-fill the optimal input\n","contour_plot_1 = ax.contourf(longitudes_tropics, latitudes_tropics, som_maps[x_node_to_plot, y_node_to_plot], \n","                             levels=np.linspace(-1,1,100), cmap=cmocean.cm.diff, transform=cartopy.crs.PlateCarree(), extend='both')        \n","\n","#Change some aspects of the figure\n","ax.set_title('Cluster ' + '(' + str(x_node_to_plot) + ',' + str(y_node_to_plot) + ')' + ' Composite', fontsize=45)\n","ax.add_feature(cartopy.feature.LAND, zorder=1, edgecolor='face', facecolor='0.5')\n","ax.set_extent([np.min(longitudes_tropics), np.max(longitudes_tropics), np.min(latitudes_tropics), np.max(latitudes_tropics)], crs=cartopy.crs.PlateCarree())\n","\n","cax = fig1.add_axes([0.06, 0.5, 0.38, 0.025])\n","cbar = fig1.colorbar(contour_plot_1, cax=cax, orientation='horizontal', ticks=np.linspace(-1,1,3))\n","cbar.ax.tick_params(labelsize=30, color='0.75', labelcolor='0.75')\n","cax.text(x=0.5, y=1.1, s='SST Anomaly ($^{\\circ}$C)', rotation=0, ha='center', va='bottom', fontsize=30)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0C_70r7FyNr1","colab_type":"text"},"source":["And now we can test to see whether there are samples associated with each node..."]},{"cell_type":"code","metadata":{"id":"AeG-7eO1yTu7","colab_type":"code","colab":{}},"source":["#Identify the node each sample is associated with\n","winners = np.array([self_organizing_map.winner(x) for x in sst])\n","\n","#Initialize the matrix that will hold the count of samples associated with\n","# each node\n","winners_matrix = np.zeros((number_x_nodes, number_y_nodes))\n","\n","#Loop through each node and count the samples\n","for i in range(number_x_nodes): #Loop through the x indices\n","  for j in range(number_y_nodes): #Loop through the y indices\n","    winners_matrix[i,j] = len(np.argwhere( (winners[:,0] == i) & (winners[:,1] == j) ))\n","\n","print('The following matrix shows the number of samples within each node of the self organizing map:')\n","\n","print(winners_matrix)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BbZbs9Fk0qkn","colab_type":"text"},"source":["We can see that for this example, a sample is associated with each node. Good!"]},{"cell_type":"markdown","metadata":{"id":"4Q9q5UupLIxG","colab_type":"text"},"source":["---\n","\n","---\n","\n","\n","# Pattern Extraction Techniques (Dimensionality Reduction)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"V6eVVu7qK4c_","colab_type":"text"},"source":["\n","\n","---\n","\n","\n","# Principal component analysis (PCA)\n","\n","Principal component analysis (PCA) has been used within the geoscientific community for decades! PCA is a way of extracting patterns from data that are linearly orthogonal. That means the vectors that describe the patterns are all orthogonal to each other, and are linear.\n","\n","You might be able to see how the requirement of linearity can introduce problems. Some problems in geoscience are linear, but others are not. If we try to describe nonlinear patterns using a linear description, we risk losing information that is important for describing the physics of the pattern.\n","\n","The most important details of PCA are:\n","\n","\n","*   Each \"principal component\" is linear and orthogonal to the others\n","*   The time-series of each principal component can be used to describe the evolution of the pattern\n","*   The full field can be reconstructed using all of the principal components, or PCA can be used to remove noise by only reconstructing the patterns using the most prevalent modes of variability\n","\n","Here is an animation of how PCA works:\n","\n","<img src=\"https://media.giphy.com/media/KRpGtfuxQgCEo/giphy.gif\" width=\"600\">\n","\n","In this animation, the red line is updating until it is oriented such that the squared difference between each point and the line is minimized. This animation depicts PCA in an iterative fashion, but the method we will use is deterministic and non-iterative.\n","\n","Once the first principal component (the red vector) is identified, you can do the same thing to identify the second component, as depicted below:\n","\n","<img src=\"https://ourcodingclub.github.io/img/PCAexample.png\" width=\"500\">\n","\n","In these two-dimensional examples, it makes sense that the second principal component is simply orthogonal to the first. However, if we have many dimensions, as is commonly the case in geoscientific data, the orientation of the principal components can be difficult to understand. We'll discuss some visualization techniques that help us understand what each component represents.\n","\n","###References\n","\n","Some examples of how principal component analysis has been used in geoscience are below:\n","\n","* Thompson, D.W.J., and J.M. Wallace, 1998: The Arctic Oscillation signature in the wintertime geopotential height and temperature fields. Geophys. Res. Lett., 25, 1297-1300.\n","* Wheeler, Matthew C., and Harry H. Hendon. \"An all-season real-time multivariate MJO index: Development of an index for monitoring and prediction.\" Monthly Weather Review 132.8 (2004): 1917-1932.\n","* Hannachi, A., I. T. Jolliffe, and D. B. Stephenson. \"Empirical orthogonal functions and related techniques in atmospheric science: A review.\" International Journal of Climatology: A Journal of the Royal Meteorological Society 27.9 (2007): 1119-1152.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"g46NkJWUkRtZ","colab_type":"text"},"source":["##Implementing PCA for Our Dataset\n","\n","We'll now do PCA on the tropical SST field and plot some of the patterns.\n","\n","We're going to code up PCA in detail so you have intuition of what the algorithm is doing. But, you should also know that there are algorithms that have been optimized for speed available through sci-kit learn, too."]},{"cell_type":"code","metadata":{"id":"Ddi7srGHtabr","colab_type":"code","colab":{}},"source":["#Standardize the sst data\n","sst_standardized = (sst - np.nanmean(sst, axis=1)[:,np.newaxis]) / np.nanstd(sst, axis=1)[:,np.newaxis]\n","\n","#Calculate the correlation matrix\n","sst_correlation = 1/len(sst)*sst_standardized.T@sst_standardized\n","\n","#Perform eigenanalysis\n","sst_eigenvalues, sst_eigenvectors = np.linalg.eig(sst_correlation)\n","\n","#Calculate principal components\n","sst_pcs = sst_standardized@sst_eigenvectors"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DKwxVWcfu3Zx","colab_type":"code","colab":{}},"source":["#Calculate principal components\n","sst_pcs = sst_standardized@sst_eigenvectors\n","\n","#Standardize the principal components\n","sst_pcs = (sst_pcs - np.mean(sst_pcs, axis=0)) / np.std(sst_pcs, axis=0)\n","\n","#Calculate the EOFs in physical units\n","sst_eigenvectors_physical = 1/len(sst)*sst_pcs.T@sst\n","\n","#Reshape the eigenvectors into the map shape\n","sst_eigenvectors_physical = sst_eigenvectors_physical.reshape(len(sst_eigenvectors_physical), len(latitudes_tropics), len(longitudes_tropics))\n","sst_eigenvectors = sst_eigenvectors.reshape(len(sst_eigenvectors), len(latitudes_tropics), len(longitudes_tropics))\n","\n","#Calculate the variance explained by each eigenvector\n","sst_eigenvalues_variance = sst_eigenvalues/np.sum(sst_eigenvalues)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9GauTrYz9VLS","colab_type":"text"},"source":["## Checking the Results of the Principal Component Analysis\n","\n","We will now plot the \"skree plot\", which is just the plot of the variances explained by each principal component. This will help us know how prevalent the dominant patterns within our data are, and will also guide our intuition as we move into more sophisticated methods of unsupervised pattern discovery."]},{"cell_type":"code","metadata":{"id":"tZ-ZjYTK9kT3","colab_type":"code","colab":{}},"source":["fig1, ax1 = plt.subplots()\n","\n","ax1.plot(sst_eigenvalues_variance[:10],'o')\n","\n","ax1.set_ylabel('Variance Explained\\n(fractional)', color='0.75', fontsize=20)\n","ax1.set_xlabel('Eigenvector Number', color='0.75', fontsize=20)\n","\n","ax1.tick_params(labelsize=15, color='0.75', labelcolor='0.75')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZheK2gGZ-i8g","colab_type":"text"},"source":["We can see that the first eigenvector explains a large amount of the variance -- about thirty percent. This is encouraging, because we know that the ENSO signal is a dominant mode of variability, so it is possible the first eigenvector is capturing the dominant ENSO patterns."]},{"cell_type":"markdown","metadata":{"id":"nNlQ_pzw9Ps7","colab_type":"text"},"source":["Now we will plot some of the eigenvectors to get a feeling for what types of structures the PCA is extracting."]},{"cell_type":"code","metadata":{"id":"Nle-T7RaoHYH","colab_type":"code","colab":{}},"source":["component_to_plot = 1\n","\n","#Axes instance for the optimal input \n","fig1 = plt.figure(figsize=(20,10))\n","\n","ax = plt.axes((0.0,0.5,0.5,0.5),  projection=cartopy.crs.Robinson(central_longitude=225))\n","\n","#Contour-fill the optimal input\n","contour_plot_1 = ax.contourf(longitudes_tropics, latitudes_tropics, sst_eigenvectors_physical[component_to_plot], \n","                             levels=np.linspace(-1,1,100), cmap=cmocean.cm.diff, transform=cartopy.crs.PlateCarree(), extend='both')        \n","\n","#Change some aspects of the figure\n","ax.set_title('Eigenvector ' + str(component_to_plot), fontsize=45)\n","ax.add_feature(cartopy.feature.LAND, zorder=1, edgecolor='face', facecolor='0.5')\n","ax.set_extent([np.min(longitudes_tropics), np.max(longitudes_tropics), np.min(latitudes_tropics), np.max(latitudes_tropics)], crs=cartopy.crs.PlateCarree())\n","\n","cax = fig1.add_axes([0.06, 0.5, 0.38, 0.025])\n","cbar = fig1.colorbar(contour_plot_1, cax=cax, orientation='horizontal', ticks=np.linspace(-1,1,3))\n","cbar.ax.tick_params(labelsize=30, color='0.75', labelcolor='0.75')\n","cax.text(x=0.5, y=1.1, s='SST Anomaly ($^{\\circ}$C)', rotation=0, ha='center', va='bottom', fontsize=30)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pc2Jj5nP_GmN","colab_type":"text"},"source":["If you plot the first eigenvector, you will notice that the pattern is very similar to the ENSO composites from our cluster analysis. This is great! That means our PCA worked.\n","\n","Cycling through the second through fourth principal components will show you patterns that explain shifts in the locations of the dominant ENSO pattern. For example, the second eigenvector introduces an eastward shifted pattern, and the third eigenvector introduces a pattern shifted toward the central Pacific.\n","\n","Now that we have an intuition for conventional pattern extraction methods, we'll move on to more modern (and in my opinion exciting) techniques."]},{"cell_type":"markdown","metadata":{"id":"o5NnPwPJOa8b","colab_type":"text"},"source":["\n","\n","---\n","\n","\n","\n","# Autoencoders\n","\n","Autoencoders are the most modern algorithm we will be discussing today. Autoencoders are neural networks that extract the dominant patterns of variability within a dataset in an unsupervised manner.\n","\n","Autoencoders can be thought of similarly as principal component analysis. In fact, a linear autoencoder leads to identical output from principal component analysis.\n","\n","The most useful aspect of autoencoders for our purposes is their ability to extract nonlinear modes of variability. For nonlinear geophysical processes, these nonlinear representations of our data can be more representative of the processes at play.\n","\n","The most important details of autoencoders are:\n","\n","*   They are neural networks\n","*   If the networks are nonlinear, then the reduced time-series are not orthogonal and are nonlinear\n","*   The reduced time-series are therefore not necessarily orthogonal, although with some trickery we can ensure we can linearly add them together to reconstruct the original time series\n","\n","\n","Here's a schematic diagram of what an autoencoder might look like:\n","\n","<br>\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1uEaaEJmyScNRw6lX770QKeEZSGtjpPIq)\n","\n","Image source: Hsieh, William W. \"Nonlinear principal component analysis by neural networks.\" Tellus A 53.5 (2001): 599-615.\n","\n","<br>\n","\n","The inputs (x) are transfered through the network toward the bottleneck layer (u), and then back outwards to the output layer (x'). The task of the autoencoder is to reduce the inputs through the hidden layer (h), into the encoded state (u), and then reconstruct as much of the original input as possible using only the encoded state as the output (x').\n","\n","\n","Here's a visualization of what nonlinear principal component analysis from autoencoders looks like for a case where the input data has 3 dimensions:\n","\n","<br>\n","\n","![alt text](https://docs.google.com/uc?export=download&id=1u4MtjvqLtUdkADL-ki2CzO-EV79p0dxy)\n","\n","Image source: Hsieh, William W. \"Nonlinear principal component analysis by neural networks.\" Tellus A 53.5 (2001): 599-615.\n","\n","<br>\n","\n","The x<sub>1</sub>, x<sub>2</sub>, and x<sub>3</sub> values denote the three dimensions of the input data. The dashed line shows the leading linear principal component from standard principal component analysis. The solid black line shows the leading nonlinear principal component from the autoencoder. You can see that the nonlinear principal component captures more of the structure of the data, which can be useful if we don't care about orthogonality.\n","\n","###References\n","\n","For more details about autoencoders in geoscience, check out these papers:\n","\n","* Hsieh, William W. \"Nonlinear principal component analysis by neural networks.\" Tellus A 53.5 (2001): 599-615.\n","\n","* Monahan, Adam Hugh. \"Nonlinear principal component analysis: Tropical Indo–Pacific sea surface temperature and sea level pressure.\" Journal of Climate 14.2 (2001): 219-233.\n","\n","* An, Soon-Il, William W. Hsieh, and Fei-Fei Jin. \"A nonlinear analysis of the ENSO cycle and its interdecadal changes.\" Journal of Climate 18.16 (2005): 3229-3239.\n","\n","* Racah, Evan, et al. \"Extremeweather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events.\" Advances in Neural Information Processing Systems. 2017."]},{"cell_type":"markdown","metadata":{"id":"A7dJvzs9EX9i","colab_type":"text"},"source":["##Defining the Autoencoder\n","\n","I am assuming you are familiar with the various types of neural network layers,and so I won't go into specifics on what these are. I will, however, focus on some other details such as regularization, that are particularly relevant to geoscience problems for physical reasons.\n","\n","We will define the autoencoder using the Keras package.\n","\n","##Defining a Linear Autoencoder\n","\n","We will also start out with a linear autoencoder, so we can show the similarities between an autoencoder's output and PCA."]},{"cell_type":"code","metadata":{"id":"bWGlAWSnE5yY","colab_type":"code","colab":{}},"source":["#Define the input layer of the neural network, which will be our SST dataset\n","# All we need to do here is tell the neural network the shape of the input\n","input_layer = keras.layers.Input(shape=(sst.shape[-1],))\n","\n","#Now we will define the encoder portion of the autoencoder\n","encoder_layer_1 = keras.layers.Dense(100, activation='linear')(input_layer)\n","encoder_layer_2 = keras.layers.Dense(50, activation='linear')(encoder_layer_1)\n","\n","#Defining the encoded layer, which is the centroid of the autoencoder\n","encoded_layer = keras.layers.Dense(1, activation='linear')(encoder_layer_2)\n","\n","#Defining the decoder portion of the autoencoder, which we will make a mirror image of\n","# the encoder portion\n","decoder_layer_1 = keras.layers.Dense(50, activation='linear')(encoded_layer)\n","decoder_layer_2 = keras.layers.Dense(100, activation='linear')(decoder_layer_1)\n","\n","#Defining the final, decoded layer\n","decoded_layer = keras.layers.Dense(sst.shape[-1], activation='linear')(decoder_layer_2)\n","\n","#Define the autoencoder. Keras automatically constructs the interior of the network\n","# so long as the input and output layers are connected through your definitions\n","# of each layer\n","autoencoder = keras.models.Model(inputs=input_layer, outputs=decoded_layer)\n","\n","#Also define the encoder branch of the network -- this object will carry the same\n","# weights as the encoder portion of the autoencoder object, but can be called\n","# separately at a later time\n","encoder = keras.models.Model(inputs=input_layer, outputs=encoded_layer)\n","\n","#Now compiling the autoencoder\n","autoencoder.compile(optimizer=keras.optimizers.SGD(lr=.1),\n","  loss = 'mse', #Our loss function is based on mean squared error\n","  metrics=[keras.metrics.mse], #We will print out the mean squared error as the network is trained\n","  )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T-_1jO3mMWzN","colab_type":"text"},"source":["We will now set some parameters that are important for when we train the model."]},{"cell_type":"code","metadata":{"id":"QX4tKbZBMZOE","colab_type":"code","colab":{}},"source":["batch_size = 32 #The number of samples the network sees before it backpropagates (batch size)\n","epochs = 50 #The number of times the network will loop through the entire dataset (epochs)\n","shuffle = True #Set whether to shuffle the training data so the model doesn't see it sequentially \n","verbose = 2 #Set whether the model will output information when trained (0 = no output; 2 = output accuracy every epoch)\n","validation_split = 0.2 #The fraction of samples we will use to validate our model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z-n6OborQXEx","colab_type":"text"},"source":["We will extract validation and training data for the autoencoder. We'll use the middle 20% of the data as validation for simplicity, but there are other methods that are useful for selecting the validation dataset, such as cross-validation."]},{"cell_type":"code","metadata":{"id":"ld41snsJQSLJ","colab_type":"code","colab":{}},"source":["#Identify the indices we would like to use for the validation dataset\n","sst_validation_indices = np.arange(int(len(sst)*0.4), int(len(sst)*0.6))\n","\n","#Extract the training and validation datasets\n","sst_training = np.delete(sst, sst_validation_indices, axis=0)\n","sst_validation = sst[sst_validation_indices]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zs3wOqJ2Kg6r","colab_type":"text"},"source":["##Training the Autoencoder\n","\n","We will now train the autoencoder.\n","\n","**Remember that each time you retrain the autoencoder, you need to recompile the autoencoder by running the cell two cells above, where we define each layer and then compile.**"]},{"cell_type":"code","metadata":{"id":"5HHh9yd7Kfds","colab_type":"code","colab":{}},"source":["autoencoder.fit(x=sst_training, y=sst_training, validation_data=(sst_validation, sst_validation), epochs=epochs, batch_size=batch_size, shuffle=shuffle)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NsuzNikRb6d6"},"source":["Okay, now let's plot what the time series of the encoded values. This is similar to plotting the principal component time series, except that we are now plotting the time series of the encoding from the autoencoder rather than the output of principal component analysis.\n","\n","We will first collect the encoding for all of the input samples..."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Mx41tmUEb6d8","colab":{}},"source":["#Gather the encoded values for each sample by \"predicting\" from only the encoder\n","# portion of the trained autoencoder\n","encoded_values = encoder.predict(sst)[:,0]\n","\n","#Standardize the encoded values for comparing to the standardized principal components\n","encoded_values = (encoded_values - np.mean(encoded_values)) / np.std(encoded_values)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dSOArOdTdA8s","colab_type":"text"},"source":["...and making the plot..."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vFMh7SnRb6d-","colab":{}},"source":["fig1, ax1 = plt.subplots(figsize=(20,4))\n","\n","ax1.plot(years, encoded_values)\n","\n","ax1.set_ylabel('Component Magnitude', fontsize=20, color='0.75')\n","ax1.set_xlabel('Year', fontsize=20, color='0.75')\n","\n","ax1.set_ylim(-4,4)\n","\n","ax1.set_xticks(np.arange(1880,2020,20))\n","ax1.set_xticklabels(np.arange(1880,2020,20))\n","\n","ax1.tick_params(labelsize=15, color='0.75', labelcolor='0.75')\n","\n","ax1.set_xlim(1880, 2018)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"If4Hb3BJcQqt"},"source":["It looks like the pattern we extracted exhibits pronounced oscillations. This is reassuring, since we know that ENSO is also oscillatory.\n","\n","### Visualizing the Nonlinear Pattern\n","\n","Let's look at the spatial pattern of the nonlinear pattern by compositing the decoded inputs. We'll composite the decoded outputs for the cases that have negative and positive encoded values separately."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"09U-l0MqcQqv","colab":{}},"source":["#Gather the decoded values by passing the inputs through the trained autoencoder\n","decoded_values = autoencoder.predict(sst)\n","\n","#Select only the negative encoded samples\n","decoded_values_pos = decoded_values[encoded_values > 0]\n","\n","#Select only the positive encoded samples\n","decoded_values_neg = decoded_values[encoded_values < 0]\n","\n","#Composite across all samples\n","decoded_values_pos_composite = np.mean(decoded_values_pos, axis=0).reshape(len(latitudes_tropics), len(longitudes_tropics))\n","decoded_values_neg_composite = np.mean(decoded_values_neg, axis=0).reshape(len(latitudes_tropics), len(longitudes_tropics))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OmrjOj55cQqy"},"source":["...and now plotting the positively encoded composite...\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YEviNWnzcQqy","colab":{}},"source":["#Axes instance for the optimal input \n","fig1 = plt.figure(figsize=(20,10))\n","\n","ax = plt.axes((0.0,0.5,0.5,0.5),  projection=cartopy.crs.Robinson(central_longitude=225))\n","\n","contour_plot_1 = ax.contourf(longitudes_tropics, latitudes_tropics, decoded_values_pos_composite, \n","                             levels=np.linspace(-1,1,100), cmap=cmocean.cm.diff, transform=cartopy.crs.PlateCarree(), extend='both')       \n","\n","#Change some aspects of the figure\n","ax.set_title('Positively Encoded Composite', fontsize=45)\n","ax.add_feature(cartopy.feature.LAND, zorder=1, edgecolor='face', facecolor='0.5')\n","ax.set_extent([np.min(longitudes_tropics), np.max(longitudes_tropics), np.min(latitudes_tropics), np.max(latitudes_tropics)], crs=cartopy.crs.PlateCarree())\n","\n","cax = fig1.add_axes([0.06, 0.5, 0.38, 0.025])\n","cbar = fig1.colorbar(contour_plot_1, cax=cax, orientation='horizontal', ticks=np.linspace(-.1,.1,3))\n","cbar.ax.tick_params(labelsize=30, color='0.75', labelcolor='0.75')\n","cax.text(x=0.5, y=1.1, s='SST Anomaly ($^{\\circ}$C)', rotation=0, ha='center', va='bottom', fontsize=30)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rFa-L3EncQq0"},"source":["...and now plotting the negatively encoded composite...\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Hh6XbNXrcQq0","colab":{}},"source":["#Axes instance for the optimal input \n","fig1 = plt.figure(figsize=(20,10))\n","\n","ax = plt.axes((0.0,0.5,0.5,0.5),  projection=cartopy.crs.Robinson(central_longitude=225))\n","\n","contour_plot_1 = ax.contourf(longitudes_tropics, latitudes_tropics, decoded_values_neg_composite, \n","                             levels=np.linspace(-1,1,100), cmap=cmocean.cm.diff, transform=cartopy.crs.PlateCarree(), extend='both')       \n","\n","#Change some aspects of the figure\n","ax.set_title('Negatively Encoded Composite', fontsize=45)\n","ax.add_feature(cartopy.feature.LAND, zorder=1, edgecolor='face', facecolor='0.5')\n","ax.set_extent([np.min(longitudes_tropics), np.max(longitudes_tropics), np.min(latitudes_tropics), np.max(latitudes_tropics)], crs=cartopy.crs.PlateCarree())\n","\n","cax = fig1.add_axes([0.06, 0.5, 0.38, 0.025])\n","cbar = fig1.colorbar(contour_plot_1, cax=cax, orientation='horizontal', ticks=np.linspace(-1,1,3))\n","cbar.ax.tick_params(labelsize=30, color='0.75', labelcolor='0.75')\n","cax.text(x=0.5, y=1.1, s='SST Anomaly ($^{\\circ}$C)', rotation=0, ha='center', va='bottom', fontsize=30)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aI5xxEuAuOM1","colab_type":"text"},"source":["Great! It looks like our autoencoder picked up on the dominant ENSO signal through its first extraction.\n","\n","The dominant pattern identified by the autoencoder is very similar to the dominant pattern identified by principal component analysis. This should be the case, since the linear autoencoder is identical to PCA.\n","\n","###Comparing the Autoencoder and Principal Component Analysis Patterns\n","\n","We'll make a quick plot below to show the similarity of the autoencoder and prinipal component analysis outputs.\n","\n","Just like in PCA, the sign of the encodings can vary between trained models and does not necessarily indicate the sign of the ENSO pattern (El Nino or La Nina).\n","\n","So, I'm going to use a quick trick to make sure we know which sign of the encoded values corresponds to El Nino and La Nina. I can do this because I know that the encoded values do correspond to the phases of ENSO, and I only do this for the sake of illustration for this session."]},{"cell_type":"code","metadata":{"id":"d_2ta8IrfxYA","colab_type":"code","colab":{}},"source":["#Identify which of the encoding signs is positive or negative by checking the sign\n","# of the average temperature anomalies within the composite\n","if np.mean(decoded_values_neg_composite) < 0:\n","  decoded_values_nina = np.copy(decoded_values_neg_composite)\n","  decoded_values_nino = np.copy(decoded_values_pos_composite)\n","else:\n","  decoded_values_nina = np.copy(decoded_values_pos_composite)\n","  decoded_values_nino = np.copy(decoded_values_neg_composite)\n","\n","#Identify which sign of ENSO the positive values of the principal component analysis\n","# correspond to\n","if np.mean(sst_eigenvectors_physical[0]) < 0:\n","  pca_enso_phase = 'La Nina'\n","else:\n","  pca_enso_phase = 'El Nino'\n","\n","#Now organize the decoded values and the prinipal component analysis to compare\n","# the El Nino values\n","if pca_enso_phase == 'El Nino':\n","  pca_el_nino = np.copy(sst_eigenvectors_physical[0])\n","  pca_la_nina = -1*np.copy(sst_eigenvectors_physical[0])\n","else:\n","  pca_el_nino = -1*np.copy(sst_eigenvectors_physical[0])\n","  pca_la_nina = np.copy(sst_eigenvectors_physical[0])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"03K2JKjcoCRF","colab_type":"text"},"source":["...and now making the plots comparing the principal component analysis and the autoencoder..."]},{"cell_type":"code","metadata":{"id":"9o2elcrBoGr6","colab_type":"code","colab":{}},"source":["#Axes instance for the optimal input \n","fig1 = plt.figure(figsize=(20,10))\n","\n","ax = plt.axes((0.0,0.5,0.5,0.5),  projection=cartopy.crs.Robinson(central_longitude=225))\n","\n","contour_plot_1 = ax.contourf(longitudes_tropics, latitudes_tropics, decoded_values_nino/np.max(decoded_values_nino) - pca_el_nino/np.max(pca_el_nino), \n","                             levels=np.linspace(-1,1,100), cmap=cmocean.cm.diff, transform=cartopy.crs.PlateCarree(), extend='both')       \n","\n","#Change some aspects of the figure\n","ax.set_title('Autoencoder minus PCA (El Nino)', fontsize=45)\n","ax.add_feature(cartopy.feature.LAND, zorder=1, edgecolor='face', facecolor='0.5')\n","ax.set_extent([np.min(longitudes_tropics), np.max(longitudes_tropics), np.min(latitudes_tropics), np.max(latitudes_tropics)], crs=cartopy.crs.PlateCarree())\n","\n","cax = fig1.add_axes([0.06, 0.5, 0.38, 0.025])\n","cbar = fig1.colorbar(contour_plot_1, cax=cax, orientation='horizontal', ticks=np.linspace(-1,1,3))\n","cbar.ax.tick_params(labelsize=30, color='0.75', labelcolor='0.75')\n","cax.text(x=0.5, y=1.1, s='SST Anomaly ($^{\\circ}$C)', rotation=0, ha='center', va='bottom', fontsize=30)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4zW7HQW_y_IL","colab_type":"text"},"source":["The differences between the two are very minor. This means that the autoencoder worked as expected, and we indeed did capture the predominant linear pattern."]},{"cell_type":"markdown","metadata":{"id":"VORtfgBhfnDo","colab_type":"text"},"source":["##Extracting Additional Patterns of Variability\n","\n","We can now extract the second most prevalent pattern by subtracting the decoded images from the original SST dataset, and training a new autoencoder on the reduced data."]},{"cell_type":"code","metadata":{"id":"7GoC_5rKuim0","colab_type":"code","colab":{}},"source":["#Remove the primary pattern from the sst dataset\n","sst_autoencoder2 = sst - decoded_values\n","\n","#And separate the modified sst dataset into training and validation data as we did\n","# for the first autoencoder\n","#Identify the indices we would like to use for the validation dataset\n","sst_validation_indices = np.arange(int(len(sst_autoencoder2)*0.4), int(len(sst_autoencoder2)*0.6))\n","\n","#Extract the training and validation datasets\n","sst_autoencoder2_training = np.delete(sst_autoencoder2, sst_validation_indices, axis=0)\n","sst_autoencoder2_validation = sst_autoencoder2[sst_validation_indices]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eaknNaSmupBv","colab_type":"text"},"source":["...now we will train a new autoencoder..."]},{"cell_type":"code","metadata":{"id":"GsJvTeeHuoX5","colab_type":"code","colab":{}},"source":["#Define the input layer of the neural network, which will be our SST dataset\n","# All we need to do here is tell the neural network the shape of the input\n","input_layer = keras.layers.Input(shape=(sst_autoencoder2.shape[-1],))\n","\n","#Now we will define the encoder portion of the autoencoder\n","encoder_layer_1 = keras.layers.Dense(10, activation='linear', \n","                                     kernel_regularizer=keras.regularizers.l1_l2(l1=0, l2=.0))(input_layer)\n","encoder_layer_2 = keras.layers.Dense(5, activation='linear',\n","                                     kernel_regularizer=keras.regularizers.l1_l2(l1=0, l2=0.0))(encoder_layer_1)\n","\n","#Defining the encoded layer, which is the centroid of the autoencoder\n","encoded_layer = keras.layers.Dense(1, activation='linear')(encoder_layer_2)\n","\n","#Defining the decoder portion of the autoencoder, which we will make a mirror image of\n","# the encoder portion\n","decoder_layer_1 = keras.layers.Dense(5, activation='linear')(encoded_layer)\n","decoder_layer_2 = keras.layers.Dense(10, activation='linear',\n","                                     kernel_regularizer=keras.regularizers.l1_l2(l1=0, l2=0.0))(decoder_layer_1)\n","\n","#Defining the final, decoded layer\n","decoded_layer = keras.layers.Dense(sst_autoencoder2.shape[-1], activation='linear',\n","                                   kernel_regularizer=keras.regularizers.l1_l2(l1=0, l2=.0))(decoder_layer_2)\n","\n","#Define the autoencoder. Keras automatically constructs the interior of the network\n","# so long as the input and output layers are connected through your definitions\n","# of each layer\n","autoencoder_2 = keras.models.Model(inputs=input_layer, outputs=decoded_layer)\n","\n","#Also define the encoder branch of the network -- this object will carry the same\n","# weights as the encoder portion of the autoencoder object, but can be called\n","# separately at a later time\n","encoder_2 = keras.models.Model(inputs=input_layer, outputs=encoded_layer)\n","\n","#Now compiling the autoencoder\n","autoencoder_2.compile(optimizer=keras.optimizers.SGD(lr=1),\n","  loss = 'mse', #Our loss function is based on mean squared error\n","  metrics=[keras.metrics.mse], #We will print out the mean squared error as the network is trained\n","  )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NKxd_s_W00rZ","colab_type":"code","colab":{}},"source":["batch_size = 32 #The number of samples the network sees before it backpropagates (batch size)\n","epochs = 50 #The number of times the network will loop through the entire dataset (epochs)\n","shuffle = True #Set whether to shuffle the training data so the model doesn't see it sequentially \n","verbose = 2 #Set whether the model will output information when trained (0 = no output; 2 = output accuracy every epoch)\n","validation_split = 0.2 #The fraction of samples we will use to validate our model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"58alysWsvBto","colab":{}},"source":["autoencoder_2.fit(x=sst_autoencoder2_training, y=sst_autoencoder2_training, validation_data=(sst_autoencoder2_validation, sst_autoencoder2_validation), epochs=epochs, batch_size=batch_size, shuffle=shuffle)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RCrPOMBXvBtq"},"source":["Okay, now let's plot what the autoencoder extracted once we removed the dominant pattern of variability.\n","\n","We will first plot the time-series of the encoded value for each sample, as we did for the first autoencoder."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CVuOSAzsvBtr","colab":{}},"source":["#Gather the encoded values for each sample by \"predicting\" from only the encoder\n","# portion of the trained autoencoder\n","encoded_values_2 = encoder_2.predict(sst_autoencoder2)[:,0]\n","\n","#Standardize the encoded values for comparing to the standardized principal components\n","encoded_values_2 = (encoded_values_2 - np.mean(encoded_values_2)) / np.std(encoded_values_2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tbEINjVovBts","colab":{}},"source":["fig1, ax1 = plt.subplots(figsize=(20,4))\n","\n","ax1.plot(years, encoded_values_2)\n","# ax1.plot(years, sst_pcs[:,1])\n","\n","ax1.set_ylabel('Component Magnitude', fontsize=20, color='0.75')\n","ax1.set_xlabel('Year', fontsize=20, color='0.75')\n","\n","ax1.set_ylim(-4,4)\n","\n","ax1.set_xticks(np.arange(1880,2020,20))\n","ax1.set_xticklabels(np.arange(1880,2020,20))\n","\n","ax1.tick_params(labelsize=15, color='0.75', labelcolor='0.75')\n","\n","ax1.set_xlim(1880, 2018)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WyN9oyU1vBtv"},"source":["You'll notice that the oscillatory pattern within the second mode of variability is higher frequency than the first has more noise. This pattern is therefore likely a higher frequency of variability overlain atop the dominant ENSO pattern, although we can't tell from this analysis whether they are directly related or not.\n","\n","### Visualizing the Secondary Pattern\n","\n","Let's look at the spatial pattern of this second mode by compositing the decoded inputs. We'll composite the decoded outputs for the cases that have negative and positive encoded values separately."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jjjwNKUBvBtw","colab":{}},"source":["#Gather the decoded values by passing the inputs through the trained autoencoder\n","decoded_values_2 = autoencoder_2.predict(sst_autoencoder2)\n","\n","#Select only the negative encoded samples\n","decoded_values_pos_2 = decoded_values_2[encoded_values_2 > 0]\n","\n","#Select only the positive encoded samples\n","decoded_values_neg_2 = decoded_values_2[encoded_values_2 < 0]\n","\n","#Composite across all samples\n","decoded_values_pos_composite_2 = np.mean(decoded_values_pos_2, axis=0).reshape(len(latitudes_tropics), len(longitudes_tropics))\n","decoded_values_neg_composite_2 = np.mean(decoded_values_neg_2, axis=0).reshape(len(latitudes_tropics), len(longitudes_tropics))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"T3ar_JhVvBtx"},"source":["...and now plotting the positively encoded composite...\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PbLRbXo4vBty","colab":{}},"source":["#Axes instance for the optimal input \n","fig1 = plt.figure(figsize=(20,10))\n","\n","ax = plt.axes((0.0,0.5,0.5,0.5),  projection=cartopy.crs.Robinson(central_longitude=225))\n","\n","contour_plot_1 = ax.contourf(longitudes_tropics, latitudes_tropics, decoded_values_pos_composite_2, \n","                             levels=np.linspace(-1,1,100), cmap=cmocean.cm.diff, transform=cartopy.crs.PlateCarree(), extend='both')       \n","\n","#Change some aspects of the figure\n","ax.set_title('Positively Encoded Composite', fontsize=45)\n","ax.add_feature(cartopy.feature.LAND, zorder=1, edgecolor='face', facecolor='0.5')\n","ax.set_extent([np.min(longitudes_tropics), np.max(longitudes_tropics), np.min(latitudes_tropics), np.max(latitudes_tropics)], crs=cartopy.crs.PlateCarree())\n","\n","cax = fig1.add_axes([0.06, 0.5, 0.38, 0.025])\n","cbar = fig1.colorbar(contour_plot_1, cax=cax, orientation='horizontal', ticks=np.linspace(-1,1,3))\n","cbar.ax.tick_params(labelsize=30, color='0.75', labelcolor='0.75')\n","cax.text(x=0.5, y=1.1, s='SST Anomaly ($^{\\circ}$C)', rotation=0, ha='center', va='bottom', fontsize=30)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"liT4hqpOpG7b","colab_type":"text"},"source":["...and now plotting the negatively encoded composite...\n"]},{"cell_type":"code","metadata":{"id":"nQHAlYOKpLBy","colab_type":"code","colab":{}},"source":["#Axes instance for the optimal input \n","fig1 = plt.figure(figsize=(20,10))\n","\n","ax = plt.axes((0.0,0.5,0.5,0.5),  projection=cartopy.crs.Robinson(central_longitude=225))\n","\n","contour_plot_1 = ax.contourf(longitudes_tropics, latitudes_tropics, decoded_values_neg_composite_2, \n","                             levels=np.linspace(-1,1,100), cmap=cmocean.cm.diff, transform=cartopy.crs.PlateCarree(), extend='both')       \n","\n","#Change some aspects of the figure\n","ax.set_title('Negatively Encoded Composite', fontsize=45)\n","ax.add_feature(cartopy.feature.LAND, zorder=1, edgecolor='face', facecolor='0.5')\n","ax.set_extent([np.min(longitudes_tropics), np.max(longitudes_tropics), np.min(latitudes_tropics), np.max(latitudes_tropics)], crs=cartopy.crs.PlateCarree())\n","\n","cax = fig1.add_axes([0.06, 0.5, 0.38, 0.025])\n","cbar = fig1.colorbar(contour_plot_1, cax=cax, orientation='horizontal', ticks=np.linspace(-1,1,3))\n","cbar.ax.tick_params(labelsize=30, color='0.75', labelcolor='0.75')\n","cax.text(x=0.5, y=1.1, s='SST Anomaly ($^{\\circ}$C)', rotation=0, ha='center', va='bottom', fontsize=30)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mm1ekCEBp6YC","colab_type":"text"},"source":["The second principal component from the autoencoder looks very similar to that from the principal component analysis. So, we can be even more confident that our autoencoder is working as intended: the linear patterns identified by the autoencoder are similar to those from principal component analysis, as theory suggests should be the case.\n","\n","We can now journey into nonlinear autoencoders.\n","\n","##Training a Nonlinear Autoencoder\n","\n","We do not have a full comparison to any other methods for the nonlinear autoencoder, and so we have to trust our physical intuition that the results are correct. Or, if we know the pattern we are trying to identify is mostly linear, we can compare the nonlinear and linear forms and check whether they are at least similar.\n","\n","We will now do the same thing as for the linear autoencoder, but introduce nonlinearity into the neurons to allow the autoencoder to identify nonlinear patterns.\n","\n","We will first define the nonlinear autoencoder."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0Tdlpw9nqlIB","colab":{}},"source":["#Define the input layer of the neural network, which will be our SST dataset\n","# All we need to do here is tell the neural network the shape of the input\n","input_layer = keras.layers.Input(shape=(sst.shape[-1],))\n","\n","#Now we will define the encoder portion of the autoencoder\n","encoder_layer_1 = keras.layers.Dense(100, activation='relu')(input_layer)\n","encoder_layer_2 = keras.layers.Dense(50, activation='relu')(encoder_layer_1)\n","\n","#Defining the encoded layer, which is the centroid of the autoencoder\n","encoded_layer = keras.layers.Dense(1, activation='linear')(encoder_layer_2)\n","\n","#Defining the decoder portion of the autoencoder, which we will make a mirror image of\n","# the encoder portion\n","decoder_layer_1 = keras.layers.Dense(50, activation='relu')(encoded_layer)\n","decoder_layer_2 = keras.layers.Dense(100, activation='relu')(decoder_layer_1)\n","\n","#Defining the final, decoded layer\n","decoded_layer = keras.layers.Dense(sst.shape[-1], activation='linear')(decoder_layer_2)\n","\n","#Define the autoencoder. Keras automatically constructs the interior of the network\n","# so long as the input and output layers are connected through your definitions\n","# of each layer\n","autoencoder_nonlinear = keras.models.Model(inputs=input_layer, outputs=decoded_layer)\n","\n","#Also define the encoder branch of the network -- this object will carry the same\n","# weights as the encoder portion of the autoencoder object, but can be called\n","# separately at a later time\n","encoder_nonlinear = keras.models.Model(inputs=input_layer, outputs=encoded_layer)\n","\n","#Now compiling the autoencoder\n","autoencoder_nonlinear.compile(optimizer=keras.optimizers.SGD(lr=1),\n","  loss = 'mse', #Our loss function is based on mean squared error\n","  metrics=[keras.metrics.mse], #We will print out the mean squared error as the network is trained\n","  )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NwPAyxZhqlIG"},"source":["We will now set some parameters that are important for when we train the model."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Vf3zJOv2qlIG","colab":{}},"source":["batch_size = 32 #The number of samples the network sees before it backpropagates (batch size)\n","epochs = 50 #The number of times the network will loop through the entire dataset (epochs)\n","shuffle = True #Set whether to shuffle the training data so the model doesn't see it sequentially \n","verbose = 2 #Set whether the model will output information when trained (0 = no output; 2 = output accuracy every epoch)\n","validation_split = 0.2 #The fraction of samples we will use to validate our model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"za7JE5qjqlIK"},"source":["We will extract validation and training data for the autoencoder. We're doing this for redundancy, but this is the same process we completed as for the linear autoencoder.\n","\n","We'll use the middle 20% of the data as validation for simplicity, but there are other methods that are useful for selecting the validation dataset, such as cross-validation."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zL_jQGzrqlIK","colab":{}},"source":["#Identify the indices we would like to use for the validation dataset\n","sst_validation_indices = np.arange(int(len(sst)*0.4), int(len(sst)*0.6))\n","\n","#Extract the training and validation datasets\n","sst_training = np.delete(sst, sst_validation_indices, axis=0)\n","sst_validation = sst[sst_validation_indices]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DW0tEuWJqlIN"},"source":["##Training the Nonlinear Autoencoder\n","\n","We will now train the nonlinear autoencoder.\n","\n","**Remember that each time you retrain the autoencoder, you need to recompile the autoencoder by running the cell two cells above, where we define each layer and then compile.**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Lojs3k8YqlIO","colab":{}},"source":["autoencoder_nonlinear.fit(x=sst_training, y=sst_training, validation_data=(sst_validation, sst_validation), epochs=epochs, batch_size=batch_size, shuffle=shuffle)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9OTY1dd1XeGl","colab_type":"text"},"source":["And now we will analyze the time series of the encoded value for each sample for this nonlinear case..."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tzr8zcs3rdWK","colab":{}},"source":["#Gather the encoded values for each sample by \"predicting\" from only the encoder\n","# portion of the trained autoencoder\n","encoded_values_nonlinear = encoder_nonlinear.predict(sst)[:,0]\n","\n","#Standardize the encoded values for comparing to the standardized principal components\n","encoded_values_nonlinear = (encoded_values_nonlinear - np.mean(encoded_values_nonlinear)) / np.std(encoded_values_nonlinear)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qsIM5GQmrdWP","colab":{}},"source":["fig1, ax1 = plt.subplots(figsize=(20,4))\n","\n","ax1.plot(years, encoded_values_nonlinear)\n","\n","ax1.set_ylabel('Component Magnitude', fontsize=20, color='0.75')\n","ax1.set_xlabel('Year', fontsize=20, color='0.75')\n","\n","ax1.set_ylim(-4,4)\n","\n","ax1.set_xticks(np.arange(1880,2020,20))\n","ax1.set_xticklabels(np.arange(1880,2020,20))\n","\n","ax1.tick_params(labelsize=15, color='0.75', labelcolor='0.75')\n","\n","ax1.set_xlim(1880, 2018)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"duRiRCCZrdWS"},"source":["We can see that there is also a pronounced oscillatory pattern within the nonlinear encoded time series. Let's plot the nonlinear and linear encoded time series to see how similar they are."]},{"cell_type":"code","metadata":{"id":"81UcsnbjXt_8","colab_type":"code","colab":{}},"source":["fig1, ax1 = plt.subplots(figsize=(20,4))\n","\n","ax1.plot(years, encoded_values_nonlinear)\n","ax1.plot(years, encoded_values)\n","ax1.plot(years, sst_pcs[:,0])\n","\n","ax1.set_ylabel('Component Magnitude', fontsize=20, color='0.75')\n","ax1.set_xlabel('Year', fontsize=20, color='0.75')\n","\n","ax1.set_ylim(-4,4)\n","\n","ax1.set_xticks(np.arange(1880,2020,20))\n","ax1.set_xticklabels(np.arange(1880,2020,20))\n","\n","ax1.tick_params(labelsize=15, color='0.75', labelcolor='0.75')\n","\n","ax1.set_xlim(1880, 2018)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KIbpLnHSXwOq","colab_type":"text"},"source":["You'll notice that the linear and nonlinear time series are very similar. This is actually what we would expect, since ENSO is mainly a linear climate pattern. But, we do know there is a little bit of nonlinearity, and so the minor differences between the time series likely reflect this nonlinearity."]},{"cell_type":"markdown","metadata":{"id":"9oz4SAjWXtcK","colab_type":"text"},"source":["### Visualizing the Nonlinear Pattern\n","\n","Let's look at the nonlinear spatial pattern. We'll eventually compare the nonlinear and linear patterns.\n","\n","Let's look at the spatial pattern of the nonlinear mode by compositing the decoded inputs. We'll again composite the decoded outputs for the cases that have negative and positive encoded values separately."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fLiBJABLrdWS","colab":{}},"source":["#Gather the decoded values by passing the inputs through the trained autoencoder\n","decoded_values_nonlinear = autoencoder_nonlinear.predict(sst)\n","\n","#Select only the negative encoded samples\n","decoded_values_pos_nonlinear = decoded_values_nonlinear[encoded_values_nonlinear > 0]\n","\n","#Select only the positive encoded samples\n","decoded_values_neg_nonlinear = decoded_values_nonlinear[encoded_values_nonlinear < 0]\n","\n","#Composite across all samples\n","decoded_values_pos_composite_nonlinear = np.mean(decoded_values_pos_nonlinear, axis=0).reshape(len(latitudes_tropics), len(longitudes_tropics))\n","decoded_values_neg_composite_nonlinear = np.mean(decoded_values_neg_nonlinear, axis=0).reshape(len(latitudes_tropics), len(longitudes_tropics))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uO80c5R9rdWU"},"source":["...and now plotting the positively encoded composite for the nonlinear autoencoder...\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6bPUSBZnrdWV","colab":{}},"source":["#Axes instance for the optimal input \n","fig1 = plt.figure(figsize=(20,10))\n","\n","ax = plt.axes((0.0,0.5,0.5,0.5),  projection=cartopy.crs.Robinson(central_longitude=225))\n","\n","contour_plot_1 = ax.contourf(longitudes_tropics, latitudes_tropics, decoded_values_pos_composite_nonlinear, \n","                             levels=np.linspace(-1,1,100), cmap=cmocean.cm.diff, transform=cartopy.crs.PlateCarree(), extend='both')       \n","\n","#Change some aspects of the figure\n","ax.set_title('Positively Encoded Composite', fontsize=45)\n","ax.add_feature(cartopy.feature.LAND, zorder=1, edgecolor='face', facecolor='0.5')\n","ax.set_extent([np.min(longitudes_tropics), np.max(longitudes_tropics), np.min(latitudes_tropics), np.max(latitudes_tropics)], crs=cartopy.crs.PlateCarree())\n","\n","cax = fig1.add_axes([0.06, 0.5, 0.38, 0.025])\n","cbar = fig1.colorbar(contour_plot_1, cax=cax, orientation='horizontal', ticks=np.linspace(-1,1,3))\n","cbar.ax.tick_params(labelsize=30, color='0.75', labelcolor='0.75')\n","cax.text(x=0.5, y=1.1, s='SST Anomaly ($^{\\circ}$C)', rotation=0, ha='center', va='bottom', fontsize=30)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hUmBurZWrdWW"},"source":["...and now plotting the negatively encoded composite for the nonlinear autoencoder...\n","\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QGfHNls7rdWX","colab":{}},"source":["#Axes instance for the optimal input \n","fig1 = plt.figure(figsize=(20,10))\n","\n","ax = plt.axes((0.0,0.5,0.5,0.5),  projection=cartopy.crs.Robinson(central_longitude=225))\n","\n","contour_plot_1 = ax.contourf(longitudes_tropics, latitudes_tropics, decoded_values_neg_composite_nonlinear, \n","                             levels=np.linspace(-1,1,100), cmap=cmocean.cm.diff, transform=cartopy.crs.PlateCarree(), extend='both')       \n","\n","#Change some aspects of the figure\n","ax.set_title('Negatively Encoded Composite', fontsize=45)\n","ax.add_feature(cartopy.feature.LAND, zorder=1, edgecolor='face', facecolor='0.5')\n","ax.set_extent([np.min(longitudes_tropics), np.max(longitudes_tropics), np.min(latitudes_tropics), np.max(latitudes_tropics)], crs=cartopy.crs.PlateCarree())\n","\n","cax = fig1.add_axes([0.06, 0.5, 0.38, 0.025])\n","cbar = fig1.colorbar(contour_plot_1, cax=cax, orientation='horizontal', ticks=np.linspace(-1,1,3))\n","cbar.ax.tick_params(labelsize=30, color='0.75', labelcolor='0.75')\n","cax.text(x=0.5, y=1.1, s='SST Anomaly ($^{\\circ}$C)', rotation=0, ha='center', va='bottom', fontsize=30)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LLQT4p4xtkhb","colab_type":"text"},"source":["## Testing the nonlinearity of the identified pattern\n","\n","We will now compare the patterns identified by the linear and nonlinear principal component analysis. We can make a crude comparison by subtracting the linear mode from the nonlinear mode and observing the difference.\n","\n","I'm going to use the same trick to identify which sign of the encoding corresponds to El Nino and La Nina so we can compare the linear and nonlinear patterns."]},{"cell_type":"code","metadata":{"id":"IphdK1iP8lrb","colab_type":"code","colab":{}},"source":["#Identify which of the encoding signs is positive or negative by checking the sign\n","# of the average temperature anomalies within the composite\n","if np.mean(decoded_values_neg_composite_nonlinear) < 0:\n","  decoded_values_nina_nonlinear = np.copy(decoded_values_neg_composite_nonlinear)\n","  decoded_values_nino_nonlinear = np.copy(decoded_values_pos_composite_nonlinear)\n","else:\n","  decoded_values_nina_nonlinear = np.copy(decoded_values_pos_composite_nonlinear)\n","  decoded_values_nino_nonlinear = np.copy(decoded_values_neg_composite_nonlinear)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MP4F8T7u9OUX","colab_type":"text"},"source":["...we will first compare the linear and nonlinear El Nino..."]},{"cell_type":"code","metadata":{"id":"ukel6zkesxui","colab_type":"code","colab":{}},"source":["#Axes instance for the optimal input \n","fig1 = plt.figure(figsize=(20,10))\n","\n","ax = plt.axes((0.0,0.5,0.5,0.5),  projection=cartopy.crs.Robinson(central_longitude=225))\n","\n","contour_plot_1 = ax.contourf(longitudes_tropics, latitudes_tropics, decoded_values_nino_nonlinear/np.max(decoded_values_nino_nonlinear) - decoded_values_nino/np.max(decoded_values_nino), \n","                             levels=np.linspace(-.5,.5,100), cmap=cmocean.cm.diff, transform=cartopy.crs.PlateCarree(), extend='both') \n","\n","#Change some aspects of the figure\n","ax.set_title('Difference Between Nonlinear and Linear Composite for El Nino', fontsize=45)\n","ax.add_feature(cartopy.feature.LAND, zorder=1, edgecolor='face', facecolor='0.5')\n","ax.set_extent([np.min(longitudes_tropics), np.max(longitudes_tropics), np.min(latitudes_tropics), np.max(latitudes_tropics)], crs=cartopy.crs.PlateCarree())\n","\n","cax = fig1.add_axes([0.06, 0.5, 0.38, 0.025])\n","cbar = fig1.colorbar(contour_plot_1, cax=cax, orientation='horizontal', ticks=np.linspace(-.5,.5,3))\n","cbar.ax.tick_params(labelsize=30, color='0.75', labelcolor='0.75')\n","cax.text(x=0.5, y=1.1, s='SST Anomaly ($^{\\circ}$C)', rotation=0, ha='center', va='bottom', fontsize=30)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oSwvu_3u9RR_","colab_type":"text"},"source":["...and now we will compare the linear and nonlinear La Nina..."]},{"cell_type":"code","metadata":{"id":"v8dLxqzT9YNm","colab_type":"code","colab":{}},"source":["#Axes instance for the optimal input \n","fig1 = plt.figure(figsize=(20,10))\n","\n","ax = plt.axes((0.0,0.5,0.5,0.5),  projection=cartopy.crs.Robinson(central_longitude=225))\n","\n","contour_plot_1 = ax.contourf(longitudes_tropics, latitudes_tropics, decoded_values_nina_nonlinear/np.max(decoded_values_nina_nonlinear) - decoded_values_nina/np.max(decoded_values_nina), \n","                             levels=np.linspace(-.5,.5,100), cmap=cmocean.cm.diff, transform=cartopy.crs.PlateCarree(), extend='both') \n","\n","#Change some aspects of the figure\n","ax.set_title('Difference Between Nonlinear and Linear Composite for El Nino', fontsize=45)\n","ax.add_feature(cartopy.feature.LAND, zorder=1, edgecolor='face', facecolor='0.5')\n","ax.set_extent([np.min(longitudes_tropics), np.max(longitudes_tropics), np.min(latitudes_tropics), np.max(latitudes_tropics)], crs=cartopy.crs.PlateCarree())\n","\n","cax = fig1.add_axes([0.06, 0.5, 0.38, 0.025])\n","cbar = fig1.colorbar(contour_plot_1, cax=cax, orientation='horizontal', ticks=np.linspace(-.5,.5,3))\n","cbar.ax.tick_params(labelsize=30, color='0.75', labelcolor='0.75')\n","cax.text(x=0.5, y=1.1, s='SST Anomaly ($^{\\circ}$C)', rotation=0, ha='center', va='bottom', fontsize=30)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KRccPNqj9GqQ","colab_type":"text"},"source":["By comparing the linear and nonlinear dominant pattern of variability, we can see that El Nino is mainly linear while La Nina exhibits some pronounced nonlinearities. This conclusion is scientifically interesting!\n","\n","In fact, autoencoders were first used back in the early 2000's when the first wave of neural network excitement arrived. There were some limitations of the study in the 2000s, though, compared to the technique that we learned here.\n","\n","For more information on autoencoders used for ENSO, check out these papers:\n","\n","Hsieh, William W. \"Nonlinear principal component analysis by neural networks.\" Tellus A 53.5 (2001): 599-615.\n","\n","An, Soon-Il, William W. Hsieh, and Fei-Fei Jin. \"A nonlinear analysis of the ENSO cycle and its interdecadal changes.\" Journal of Climate 18.16 (2005): 3229-3239."]},{"cell_type":"markdown","metadata":{"id":"8IGPaeBP4f2d","colab_type":"text"},"source":["#Interactive Section\n","\n","Now that we've covered the basics of pattern separation and extraction methods, we're going to do the same thing but with the global sea-surface temperature fields rather than the tropical sea-surface temperature.\n","\n","I don't have a perfect answer for what you should find in this example. This is mostly your chance to feel out the parameter choices for the autoencoder and reproduce what you find from the PCA and clustering algorithms.\n","\n","I'll prep the data for you, and then this is your journey to explore the methods!\n","\n","**You might want to reset your notebook before continuing to reset your RAM so your code will faster.**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WbEExSIUMRC0"},"source":["\n","\n","---\n","\n","#Installing and importing Packages\n","\n","\n","We will first install packages that we need for this tutorial."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"euMyEnRsMRC2","colab":{}},"source":["!pip install netcdf4 #Package for loading in netcdf4 files\n","!pip install cmocean #Package with beautiful colormaps\n","!pip install minisom #Package for self organizing maps\n","\n","#All of these installs are for installing the \"cartopy\" package, which is helpful for plotting data on the globe\n","!apt-get install libproj-dev proj-data proj-bin\n","!apt-get install libgeos-dev\n","!pip install cython\n","!pip install cartopy"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yYEA-ly5MRC4"},"source":["Now, we'll import some packages that we'll use during various stages of the tutorial. I've broken down each package by what it is useful for."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vm-5KUVTMRC4","colab":{}},"source":["#General Python math functions\n","import math\n","\n","#Loading in data (netcdf files)\n","import xarray as xr\n","\n","#Handling data\n","import numpy as np\n","\n","#Plotting figures\n","import matplotlib.pyplot as plt #Main plotting package\n","from matplotlib import rcParams #For changing text properties\n","import cmocean #A package with beautiful colormaps\n","import cartopy #Useful for plotting maps\n","import cartopy.util #Requires separate import\n","\n","#Making neural networks; Ensure we are using tensorflow 1.15\n","%tensorflow_version 1.x\n","import keras\n","\n","#Non-neural network machine learning/unsupervised learning algorithms\n","import sklearn.cluster\n","import scipy.cluster\n","import sklearn.decomposition\n","\n","#Self organizing maps\n","from minisom import MiniSom    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KJWVQou6MRC6"},"source":["#Removing Auto-Scroll\n","\n","Output cells will automatically scroll through their entire output unless we use the following code:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VENGxlLHMRC6","colab":{}},"source":["%%javascript\n","IPython.OutputArea.prototype._should_scroll = function(lines) {\n","    return false;\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"C00nzReSMRDE"},"source":["#Downloading Data\n","\n","We'll now download the data from a remote server and temporarily store it on Google Colab."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tQvmdtApMRDF","colab":{}},"source":["!wget http://portal.nersc.gov/project/dasrepo/AGU_ML_Tutorial/sst.mon.mean.trefadj.anom.detrend.1880to2018.nc\n","!wget http://portal.nersc.gov/project/dasrepo/AGU_ML_Tutorial/nino34.long.anom.data.txt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JaJOrtkaMaY6"},"source":["\n","\n","\n","#Data Processing (Prior to Training)\n","\n","\n","Okay! Now it's time to start processing our data.\n","\n","We will first load the datasets using xarray and numpy.\n","\n","We will also pre-process the data before training the neural network. Remember that our data has already been processed to remove climatology and any linear trends, so our job here is easy."]},{"cell_type":"code","metadata":{"id":"C5haY98X9Sc4","colab_type":"code","colab":{}},"source":["#Load in the sea-surface temperature data\n","sst_dataset = xr.open_dataset('sst.mon.mean.trefadj.anom.detrend.1880to2018.nc')\n","sst = np.array(sst_dataset['sst'])\n","latitudes = np.array(sst_dataset['lat'])\n","longitudes = np.array(sst_dataset['lon'])\n","years = np.linspace(1880, 2019, 12*139 + 1)[:-1] #Create an array for the year of each sample\n","\n","#Load in the Nino3.4 index\n","nino_34 = np.loadtxt('nino34.long.anom.data.txt')\n","\n","#Extract only the tropical Pacific latitudes and longitudes\n","#   We will use latitudes of 30S to 30N and longitudes of 105E to 300E\n","latitude_min = np.argmin(np.abs(latitudes - 30))\n","latitude_max = np.argmin(np.abs(latitudes - -30))\n","longitude_min = np.argmin(np.abs(longitudes - 105))\n","longitude_max = np.argmin(np.abs(longitudes - 300))\n","\n","#Sparsify the sea-surface temperature data by selecting every four points\n","sst = sst[:,::4,::4]\n","\n","#Vectorize the sea-surface temperature data by flattening the matrix along the spatial domains.\n","#   We vectorize the data because the algorithms we will be using all operate on 1-D data\n","sst = sst.reshape(sst.shape[0], sst.shape[-2]*sst.shape[-1])\n","\n","#The sst data has 'nan' values where there is land, but we can not train a network with data that has 'nan values\n","# So, we will replace all 'nan' values with zeros\n","sst[np.isnan(sst)] = 0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IsdzmxofMi6W","colab_type":"text"},"source":["Now get creative with code! Some suggestions:\n","\n","\n","\n","*   Mimic what we did with the tropical sea-surface temperature data. Compare the linear and nonlinear autoencoder output to test whether the global patterns of sea-surface temperature are more nonlinear than the tropical patterns.\n","*   Change the domains of the sea-surface temperature data and assess the nonlinearity of different regions (e.g. the North Pacific, the tropics, and the North Atlantic)\n","\n"]},{"cell_type":"code","metadata":{"id":"2vq6T7L-NmA9","colab_type":"code","colab":{}},"source":["#Your code starts here"],"execution_count":0,"outputs":[]}]}